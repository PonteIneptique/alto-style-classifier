{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         _ : 0.91 of the whole (38663)\n",
      "   italics : 0.04 of the whole (1731)\n",
      "      bold : 0.05 of the whole (1928)\n",
      "1731\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from collections import namedtuple, defaultdict\n",
    "import lxml.etree as et\n",
    "import os\n",
    "\n",
    "class NoSourceImage(Exception):\n",
    "    \"\"\" Raised when the ALTO is missing a link to an image file \"\"\"\n",
    "\n",
    "BBOX = namedtuple(\"Bbox\", [\"x1\", \"y1\", \"x2\", \"y2\", \"file\", \"id\", \"image\"])\n",
    "NS = {\"a\": \"http://www.loc.gov/standards/alto/ns-v2#\"}\n",
    "\n",
    "def temporary_replace_path(xml_path):\n",
    "    return f\"../IMG/{xml_path.replace('.xml', '.jpg')}\"\n",
    "\n",
    "def read_alto(alto_xml) -> Tuple[Dict[str, List[BBOX]], str]:\n",
    "    classes = defaultdict(list)\n",
    "    \n",
    "    with open(alto_xml) as f:\n",
    "        xml = et.parse(f)\n",
    "        source_image = xml.xpath(\"//a:sourceImageInformation/a:fileName/text()\", namespaces=NS)\n",
    "        if not len(source_image):\n",
    "            raise NoSourceImage(f\"{alto_xml} is missing the following node\"\n",
    "                                \"`/alto/Description/sourceImageInformation/fileName`\"\n",
    "                               \"which should contain the path to the image it is about\")\n",
    "        source_image = temporary_replace_path(source_image[0])\n",
    "        source_image_real_path = os.path.abspath(\n",
    "            os.path.join(os.path.dirname(alto_xml), source_image)\n",
    "        )\n",
    "        if not os.path.isfile(source_image_real_path):\n",
    "            raise NoSourceImage(f\"{alto_xml} has a wrong path at\"\n",
    "                                \"`/alto/Description/sourceImageInformation/fileName`\"\n",
    "                               f\": {source_image_real_path}\")\n",
    "        \n",
    "        styles = {\n",
    "            style.attrib[\"ID\"]: style.attrib[\"FONTSTYLE\"] if style.attrib[\"FONTSTYLE\"] else \"_\"\n",
    "            for style in xml.xpath(\"//a:TextStyle\", namespaces=NS)\n",
    "        }\n",
    "        for string in xml.xpath(\"//a:String\", namespaces=NS):\n",
    "            x, y, = string.attrib[\"HPOS\"], string.attrib[\"VPOS\"]\n",
    "            w, h = string.attrib[\"WIDTH\"], string.attrib[\"HEIGHT\"]\n",
    "            x, y, w, h = float(x), float(y), float(w), float(h)\n",
    "            style = styles[string.attrib[\"STYLEREFS\"]]\n",
    "            classes[style].append(BBOX(x, y, x+w, y+h, alto_xml, \n",
    "                                       string.attrib[\"ID\"], source_image_real_path))\n",
    "            \n",
    "    return classes, source_image_real_path\n",
    "\n",
    "import glob\n",
    "\n",
    "data = defaultdict(list)\n",
    "images = {}\n",
    "for xml_path in glob.glob(\"./input/*/ALTO/*.xml\"):\n",
    "    current, image = read_alto(xml_path)\n",
    "    images[image] = current\n",
    "    for key in current:\n",
    "        data[key].extend(current[key])\n",
    "    \n",
    "minimum = float(\"inf\")\n",
    "for cls in data:\n",
    "    total = sum([len(val) for val in data.values()])\n",
    "    print(f\"{cls.zfill(10).replace('0', ' ')} : {len(data[cls])/total:.2f} of the whole ({len(data[cls])})\")\n",
    "    minimum = min([len(data[cls]), minimum])\n",
    "print(minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display\n",
    "import PIL.Image as PILImage\n",
    "#import tqdm.tqdm\n",
    "\n",
    "output_dir = \"./data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for image, bboxes in images.items():\n",
    "    for cls, items in bboxes.items():\n",
    "        source = PILImage.open(image)\n",
    "        os.makedirs(os.path.join(output_dir, cls), exist_ok=True)\n",
    "        for id_, bbox in enumerate(items):\n",
    "            area = source.crop(bbox[:4])\n",
    "            area.save(\n",
    "                os.path.join(\n",
    "                    output_dir,\n",
    "                    cls,\n",
    "                    f\"{os.path.basename(bbox.image)}.{id_}.png\"\n",
    "                )\n",
    "            ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and generate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test_dir = \"./test_data\"\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "for cls in glob.glob(\"./data/*\"):\n",
    "    files = glob.glob(os.path.join(cls, \"*.png\"))\n",
    "    random.shuffle(files)\n",
    "    os.makedirs(cls.replace(\"data\", \"test_data\"), exist_ok=True)\n",
    "    for file in files[:int(minimum*0.1)]:\n",
    "        os.rename(file, file.replace(\"data\", \"test_data\"))\n",
    "        \n",
    "train_dir = \"./train_data\"\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "for cls in glob.glob(\"./data/*\"):\n",
    "    files = glob.glob(os.path.join(cls, \"*.png\"))\n",
    "    random.shuffle(files)\n",
    "    os.makedirs(cls.replace(\"data\", train_dir), exist_ok=True)\n",
    "    for file in files[:minimum]:\n",
    "        os.rename(file, file.replace(\"data\", train_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: 'bold', 2: 'italics'}\n",
      "{0: '_', 1: 'bold', 2: 'italics'}\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "\n",
    "pre_process = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        #transforms.Resize((28, 28)),\n",
    "        transforms.CenterCrop((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transform=pre_process\n",
    ")\n",
    "testset = torchvision.datasets.ImageFolder(\n",
    "    test_dir,\n",
    "    transform=pre_process\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=4,\n",
    "    shuffle=True, num_workers=2\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=4,\n",
    "    shuffle=True, num_workers=2\n",
    ")\n",
    "classes = {n: key for n, key in enumerate(trainset.classes)}\n",
    "test_classes = {n: key for n, key in enumerate(testset.classes)}\n",
    "print(classes)\n",
    "print(test_classes)\n",
    "assert classes == test_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes    _     _ italics  bold\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(\"Classes\" + ' '.join('%5s' % classes[labels[j].tolist()] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count classes in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'italics': 173, '_': 173, 'bold': 173})\n"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "from collections import Counter\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "\n",
    "out = []\n",
    "for images, labels in dataiter:\n",
    "    out.extend([classes[labels[j].tolist()] for j in range(len(labels))])\n",
    "    \n",
    "print(Counter(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, classes: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv1_drop = nn.Dropout2d()\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "import torch.optim as optim\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "random_seed = 1\n",
    "\n",
    "network = Net(len(classes))\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(trainloader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(trainloader.dataset)))\n",
    "            torch.save(network.state_dict(), './results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), './results/optimizer.pth')\n",
    "    lr_scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(testloader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(testloader.dataset),\n",
    "    100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/dev/alto-fontstyle-classifier/env/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.1029, Accuracy: 173/519 (33%)\n",
      "\n",
      "Train Epoch: 1 [0/5020 (0%)]\tLoss: 1.119196\n",
      "Train Epoch: 1 [400/5020 (8%)]\tLoss: 1.111358\n",
      "Train Epoch: 1 [800/5020 (16%)]\tLoss: 1.104193\n",
      "Train Epoch: 1 [1200/5020 (24%)]\tLoss: 1.131489\n",
      "Train Epoch: 1 [1600/5020 (32%)]\tLoss: 1.099206\n",
      "Train Epoch: 1 [2000/5020 (40%)]\tLoss: 1.063928\n",
      "Train Epoch: 1 [2400/5020 (48%)]\tLoss: 1.135306\n",
      "Train Epoch: 1 [2800/5020 (56%)]\tLoss: 1.080824\n",
      "Train Epoch: 1 [3200/5020 (64%)]\tLoss: 1.056047\n",
      "Train Epoch: 1 [3600/5020 (72%)]\tLoss: 1.132410\n",
      "Train Epoch: 1 [4000/5020 (80%)]\tLoss: 1.093631\n",
      "Train Epoch: 1 [4400/5020 (88%)]\tLoss: 1.094402\n",
      "Train Epoch: 1 [4800/5020 (96%)]\tLoss: 1.106431\n",
      "\n",
      "Test set: Avg. loss: 1.0948, Accuracy: 232/519 (45%)\n",
      "\n",
      "Train Epoch: 2 [0/5020 (0%)]\tLoss: 1.094679\n",
      "Train Epoch: 2 [400/5020 (8%)]\tLoss: 1.077314\n",
      "Train Epoch: 2 [800/5020 (16%)]\tLoss: 1.084013\n",
      "Train Epoch: 2 [1200/5020 (24%)]\tLoss: 1.100687\n",
      "Train Epoch: 2 [1600/5020 (32%)]\tLoss: 1.103486\n",
      "Train Epoch: 2 [2000/5020 (40%)]\tLoss: 1.108713\n",
      "Train Epoch: 2 [2400/5020 (48%)]\tLoss: 1.082175\n",
      "Train Epoch: 2 [2800/5020 (56%)]\tLoss: 1.085491\n",
      "Train Epoch: 2 [3200/5020 (64%)]\tLoss: 1.142704\n",
      "Train Epoch: 2 [3600/5020 (72%)]\tLoss: 1.083164\n",
      "Train Epoch: 2 [4000/5020 (80%)]\tLoss: 1.131860\n",
      "Train Epoch: 2 [4400/5020 (88%)]\tLoss: 1.107821\n",
      "Train Epoch: 2 [4800/5020 (96%)]\tLoss: 1.116673\n",
      "\n",
      "Test set: Avg. loss: 1.0878, Accuracy: 253/519 (49%)\n",
      "\n",
      "Train Epoch: 3 [0/5020 (0%)]\tLoss: 1.152397\n",
      "Train Epoch: 3 [400/5020 (8%)]\tLoss: 1.048033\n",
      "Train Epoch: 3 [800/5020 (16%)]\tLoss: 1.112013\n",
      "Train Epoch: 3 [1200/5020 (24%)]\tLoss: 1.084738\n",
      "Train Epoch: 3 [1600/5020 (32%)]\tLoss: 1.125212\n",
      "Train Epoch: 3 [2000/5020 (40%)]\tLoss: 1.095508\n",
      "Train Epoch: 3 [2400/5020 (48%)]\tLoss: 1.077920\n",
      "Train Epoch: 3 [2800/5020 (56%)]\tLoss: 1.122609\n",
      "Train Epoch: 3 [3200/5020 (64%)]\tLoss: 1.072522\n",
      "Train Epoch: 3 [3600/5020 (72%)]\tLoss: 1.172673\n",
      "Train Epoch: 3 [4000/5020 (80%)]\tLoss: 1.043879\n",
      "Train Epoch: 3 [4400/5020 (88%)]\tLoss: 1.004683\n",
      "Train Epoch: 3 [4800/5020 (96%)]\tLoss: 1.157544\n",
      "\n",
      "Test set: Avg. loss: 1.0595, Accuracy: 278/519 (54%)\n",
      "\n",
      "Train Epoch: 4 [0/5020 (0%)]\tLoss: 1.108490\n",
      "Train Epoch: 4 [400/5020 (8%)]\tLoss: 1.069933\n",
      "Train Epoch: 4 [800/5020 (16%)]\tLoss: 1.088226\n",
      "Train Epoch: 4 [1200/5020 (24%)]\tLoss: 0.808639\n",
      "Train Epoch: 4 [1600/5020 (32%)]\tLoss: 1.078947\n",
      "Train Epoch: 4 [2000/5020 (40%)]\tLoss: 1.149608\n",
      "Train Epoch: 4 [2400/5020 (48%)]\tLoss: 0.986695\n",
      "Train Epoch: 4 [2800/5020 (56%)]\tLoss: 1.075858\n",
      "Train Epoch: 4 [3200/5020 (64%)]\tLoss: 1.133556\n",
      "Train Epoch: 4 [3600/5020 (72%)]\tLoss: 1.014777\n",
      "Train Epoch: 4 [4000/5020 (80%)]\tLoss: 0.790317\n",
      "Train Epoch: 4 [4400/5020 (88%)]\tLoss: 0.975060\n",
      "Train Epoch: 4 [4800/5020 (96%)]\tLoss: 1.076799\n",
      "\n",
      "Test set: Avg. loss: 0.9759, Accuracy: 282/519 (54%)\n",
      "\n",
      "Train Epoch: 5 [0/5020 (0%)]\tLoss: 1.154817\n",
      "Train Epoch: 5 [400/5020 (8%)]\tLoss: 1.040177\n",
      "Train Epoch: 5 [800/5020 (16%)]\tLoss: 0.803512\n",
      "Train Epoch: 5 [1200/5020 (24%)]\tLoss: 0.949617\n",
      "Train Epoch: 5 [1600/5020 (32%)]\tLoss: 0.966105\n",
      "Train Epoch: 5 [2000/5020 (40%)]\tLoss: 0.801315\n",
      "Train Epoch: 5 [2400/5020 (48%)]\tLoss: 0.964870\n",
      "Train Epoch: 5 [2800/5020 (56%)]\tLoss: 1.041401\n",
      "Train Epoch: 5 [3200/5020 (64%)]\tLoss: 1.035780\n",
      "Train Epoch: 5 [3600/5020 (72%)]\tLoss: 1.113137\n",
      "Train Epoch: 5 [4000/5020 (80%)]\tLoss: 1.091166\n",
      "Train Epoch: 5 [4400/5020 (88%)]\tLoss: 1.218906\n",
      "Train Epoch: 5 [4800/5020 (96%)]\tLoss: 1.133307\n",
      "\n",
      "Test set: Avg. loss: 0.8854, Accuracy: 303/519 (58%)\n",
      "\n",
      "Train Epoch: 6 [0/5020 (0%)]\tLoss: 0.915836\n",
      "Train Epoch: 6 [400/5020 (8%)]\tLoss: 0.828432\n",
      "Train Epoch: 6 [800/5020 (16%)]\tLoss: 0.968135\n",
      "Train Epoch: 6 [1200/5020 (24%)]\tLoss: 0.714559\n",
      "Train Epoch: 6 [1600/5020 (32%)]\tLoss: 0.837340\n",
      "Train Epoch: 6 [2000/5020 (40%)]\tLoss: 0.749472\n",
      "Train Epoch: 6 [2400/5020 (48%)]\tLoss: 0.858543\n",
      "Train Epoch: 6 [2800/5020 (56%)]\tLoss: 0.805146\n",
      "Train Epoch: 6 [3200/5020 (64%)]\tLoss: 0.985301\n",
      "Train Epoch: 6 [3600/5020 (72%)]\tLoss: 1.083002\n",
      "Train Epoch: 6 [4000/5020 (80%)]\tLoss: 0.959400\n",
      "Train Epoch: 6 [4400/5020 (88%)]\tLoss: 0.846543\n",
      "Train Epoch: 6 [4800/5020 (96%)]\tLoss: 1.049045\n",
      "\n",
      "Test set: Avg. loss: 0.8197, Accuracy: 309/519 (60%)\n",
      "\n",
      "Train Epoch: 7 [0/5020 (0%)]\tLoss: 0.633536\n",
      "Train Epoch: 7 [400/5020 (8%)]\tLoss: 0.788916\n",
      "Train Epoch: 7 [800/5020 (16%)]\tLoss: 1.025272\n",
      "Train Epoch: 7 [1200/5020 (24%)]\tLoss: 0.978894\n",
      "Train Epoch: 7 [1600/5020 (32%)]\tLoss: 0.816119\n",
      "Train Epoch: 7 [2000/5020 (40%)]\tLoss: 1.004142\n",
      "Train Epoch: 7 [2400/5020 (48%)]\tLoss: 0.615418\n",
      "Train Epoch: 7 [2800/5020 (56%)]\tLoss: 1.038530\n",
      "Train Epoch: 7 [3200/5020 (64%)]\tLoss: 0.553675\n",
      "Train Epoch: 7 [3600/5020 (72%)]\tLoss: 0.757799\n",
      "Train Epoch: 7 [4000/5020 (80%)]\tLoss: 0.762100\n",
      "Train Epoch: 7 [4400/5020 (88%)]\tLoss: 1.019975\n",
      "Train Epoch: 7 [4800/5020 (96%)]\tLoss: 0.809217\n",
      "\n",
      "Test set: Avg. loss: 0.7674, Accuracy: 314/519 (61%)\n",
      "\n",
      "Train Epoch: 8 [0/5020 (0%)]\tLoss: 0.882387\n",
      "Train Epoch: 8 [400/5020 (8%)]\tLoss: 0.591220\n",
      "Train Epoch: 8 [800/5020 (16%)]\tLoss: 0.832950\n",
      "Train Epoch: 8 [1200/5020 (24%)]\tLoss: 0.968915\n",
      "Train Epoch: 8 [1600/5020 (32%)]\tLoss: 1.265511\n",
      "Train Epoch: 8 [2000/5020 (40%)]\tLoss: 0.731226\n",
      "Train Epoch: 8 [2400/5020 (48%)]\tLoss: 0.647088\n",
      "Train Epoch: 8 [2800/5020 (56%)]\tLoss: 0.492436\n",
      "Train Epoch: 8 [3200/5020 (64%)]\tLoss: 0.589965\n",
      "Train Epoch: 8 [3600/5020 (72%)]\tLoss: 0.763003\n",
      "Train Epoch: 8 [4000/5020 (80%)]\tLoss: 0.982511\n",
      "Train Epoch: 8 [4400/5020 (88%)]\tLoss: 1.465243\n",
      "Train Epoch: 8 [4800/5020 (96%)]\tLoss: 1.399187\n",
      "\n",
      "Test set: Avg. loss: 0.7368, Accuracy: 332/519 (64%)\n",
      "\n",
      "Train Epoch: 9 [0/5020 (0%)]\tLoss: 1.386464\n",
      "Train Epoch: 9 [400/5020 (8%)]\tLoss: 0.640410\n",
      "Train Epoch: 9 [800/5020 (16%)]\tLoss: 1.094198\n",
      "Train Epoch: 9 [1200/5020 (24%)]\tLoss: 0.971833\n",
      "Train Epoch: 9 [1600/5020 (32%)]\tLoss: 1.195872\n",
      "Train Epoch: 9 [2000/5020 (40%)]\tLoss: 0.984018\n",
      "Train Epoch: 9 [2400/5020 (48%)]\tLoss: 0.555126\n",
      "Train Epoch: 9 [2800/5020 (56%)]\tLoss: 0.462482\n",
      "Train Epoch: 9 [3200/5020 (64%)]\tLoss: 0.897552\n",
      "Train Epoch: 9 [3600/5020 (72%)]\tLoss: 0.785588\n",
      "Train Epoch: 9 [4000/5020 (80%)]\tLoss: 0.540138\n",
      "Train Epoch: 9 [4400/5020 (88%)]\tLoss: 0.848082\n",
      "Train Epoch: 9 [4800/5020 (96%)]\tLoss: 0.895920\n",
      "\n",
      "Test set: Avg. loss: 0.7089, Accuracy: 339/519 (65%)\n",
      "\n",
      "Train Epoch: 10 [0/5020 (0%)]\tLoss: 0.629908\n",
      "Train Epoch: 10 [400/5020 (8%)]\tLoss: 0.927326\n",
      "Train Epoch: 10 [800/5020 (16%)]\tLoss: 0.686315\n",
      "Train Epoch: 10 [1200/5020 (24%)]\tLoss: 1.106481\n",
      "Train Epoch: 10 [1600/5020 (32%)]\tLoss: 0.593657\n",
      "Train Epoch: 10 [2000/5020 (40%)]\tLoss: 0.737040\n",
      "Train Epoch: 10 [2400/5020 (48%)]\tLoss: 0.663834\n",
      "Train Epoch: 10 [2800/5020 (56%)]\tLoss: 1.026103\n",
      "Train Epoch: 10 [3200/5020 (64%)]\tLoss: 0.411864\n",
      "Train Epoch: 10 [3600/5020 (72%)]\tLoss: 0.891077\n",
      "Train Epoch: 10 [4000/5020 (80%)]\tLoss: 0.686078\n",
      "Train Epoch: 10 [4400/5020 (88%)]\tLoss: 1.396645\n",
      "Train Epoch: 10 [4800/5020 (96%)]\tLoss: 1.058237\n",
      "\n",
      "Test set: Avg. loss: 0.7011, Accuracy: 315/519 (61%)\n",
      "\n",
      "Train Epoch: 11 [0/5020 (0%)]\tLoss: 0.713037\n",
      "Train Epoch: 11 [400/5020 (8%)]\tLoss: 0.621640\n",
      "Train Epoch: 11 [800/5020 (16%)]\tLoss: 1.039179\n",
      "Train Epoch: 11 [1200/5020 (24%)]\tLoss: 0.665109\n",
      "Train Epoch: 11 [1600/5020 (32%)]\tLoss: 0.998372\n",
      "Train Epoch: 11 [2000/5020 (40%)]\tLoss: 0.971230\n",
      "Train Epoch: 11 [2400/5020 (48%)]\tLoss: 0.863502\n",
      "Train Epoch: 11 [2800/5020 (56%)]\tLoss: 0.668341\n",
      "Train Epoch: 11 [3200/5020 (64%)]\tLoss: 0.421705\n",
      "Train Epoch: 11 [3600/5020 (72%)]\tLoss: 1.076738\n",
      "Train Epoch: 11 [4000/5020 (80%)]\tLoss: 0.699445\n",
      "Train Epoch: 11 [4400/5020 (88%)]\tLoss: 0.828969\n",
      "Train Epoch: 11 [4800/5020 (96%)]\tLoss: 0.679544\n",
      "\n",
      "Test set: Avg. loss: 0.6721, Accuracy: 341/519 (66%)\n",
      "\n",
      "Train Epoch: 12 [0/5020 (0%)]\tLoss: 0.809859\n",
      "Train Epoch: 12 [400/5020 (8%)]\tLoss: 0.660554\n",
      "Train Epoch: 12 [800/5020 (16%)]\tLoss: 0.983737\n",
      "Train Epoch: 12 [1200/5020 (24%)]\tLoss: 1.468799\n",
      "Train Epoch: 12 [1600/5020 (32%)]\tLoss: 0.909602\n",
      "Train Epoch: 12 [2000/5020 (40%)]\tLoss: 0.589049\n",
      "Train Epoch: 12 [2400/5020 (48%)]\tLoss: 0.737749\n",
      "Train Epoch: 12 [2800/5020 (56%)]\tLoss: 0.729002\n",
      "Train Epoch: 12 [3200/5020 (64%)]\tLoss: 1.018298\n",
      "Train Epoch: 12 [3600/5020 (72%)]\tLoss: 0.700978\n",
      "Train Epoch: 12 [4000/5020 (80%)]\tLoss: 0.778532\n",
      "Train Epoch: 12 [4400/5020 (88%)]\tLoss: 0.844927\n",
      "Train Epoch: 12 [4800/5020 (96%)]\tLoss: 0.622658\n",
      "\n",
      "Test set: Avg. loss: 0.6441, Accuracy: 336/519 (65%)\n",
      "\n",
      "Train Epoch: 13 [0/5020 (0%)]\tLoss: 0.572790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [400/5020 (8%)]\tLoss: 0.874925\n",
      "Train Epoch: 13 [800/5020 (16%)]\tLoss: 0.594800\n",
      "Train Epoch: 13 [1200/5020 (24%)]\tLoss: 0.927224\n",
      "Train Epoch: 13 [1600/5020 (32%)]\tLoss: 0.711652\n",
      "Train Epoch: 13 [2000/5020 (40%)]\tLoss: 0.486497\n",
      "Train Epoch: 13 [2400/5020 (48%)]\tLoss: 0.383975\n",
      "Train Epoch: 13 [2800/5020 (56%)]\tLoss: 0.616082\n",
      "Train Epoch: 13 [3200/5020 (64%)]\tLoss: 1.141925\n",
      "Train Epoch: 13 [3600/5020 (72%)]\tLoss: 0.618363\n",
      "Train Epoch: 13 [4000/5020 (80%)]\tLoss: 0.659854\n",
      "Train Epoch: 13 [4400/5020 (88%)]\tLoss: 1.323017\n",
      "Train Epoch: 13 [4800/5020 (96%)]\tLoss: 0.759155\n",
      "\n",
      "Test set: Avg. loss: 0.6320, Accuracy: 356/519 (69%)\n",
      "\n",
      "Train Epoch: 14 [0/5020 (0%)]\tLoss: 1.033715\n",
      "Train Epoch: 14 [400/5020 (8%)]\tLoss: 0.596660\n",
      "Train Epoch: 14 [800/5020 (16%)]\tLoss: 0.345134\n",
      "Train Epoch: 14 [1200/5020 (24%)]\tLoss: 0.198671\n",
      "Train Epoch: 14 [1600/5020 (32%)]\tLoss: 0.579176\n",
      "Train Epoch: 14 [2000/5020 (40%)]\tLoss: 0.662268\n",
      "Train Epoch: 14 [2400/5020 (48%)]\tLoss: 0.772731\n",
      "Train Epoch: 14 [2800/5020 (56%)]\tLoss: 1.300158\n",
      "Train Epoch: 14 [3200/5020 (64%)]\tLoss: 0.631445\n",
      "Train Epoch: 14 [3600/5020 (72%)]\tLoss: 0.430600\n",
      "Train Epoch: 14 [4000/5020 (80%)]\tLoss: 0.599191\n",
      "Train Epoch: 14 [4400/5020 (88%)]\tLoss: 1.241336\n",
      "Train Epoch: 14 [4800/5020 (96%)]\tLoss: 1.120367\n",
      "\n",
      "Test set: Avg. loss: 0.6295, Accuracy: 375/519 (72%)\n",
      "\n",
      "Train Epoch: 15 [0/5020 (0%)]\tLoss: 0.589624\n",
      "Train Epoch: 15 [400/5020 (8%)]\tLoss: 0.999313\n",
      "Train Epoch: 15 [800/5020 (16%)]\tLoss: 0.619609\n",
      "Train Epoch: 15 [1200/5020 (24%)]\tLoss: 0.851480\n",
      "Train Epoch: 15 [1600/5020 (32%)]\tLoss: 0.774202\n",
      "Train Epoch: 15 [2000/5020 (40%)]\tLoss: 0.605134\n",
      "Train Epoch: 15 [2400/5020 (48%)]\tLoss: 0.478072\n",
      "Train Epoch: 15 [2800/5020 (56%)]\tLoss: 0.664294\n",
      "Train Epoch: 15 [3200/5020 (64%)]\tLoss: 0.480513\n",
      "Train Epoch: 15 [3600/5020 (72%)]\tLoss: 0.597179\n",
      "Train Epoch: 15 [4000/5020 (80%)]\tLoss: 0.535859\n",
      "Train Epoch: 15 [4400/5020 (88%)]\tLoss: 0.509641\n",
      "Train Epoch: 15 [4800/5020 (96%)]\tLoss: 0.719568\n",
      "\n",
      "Test set: Avg. loss: 0.6062, Accuracy: 376/519 (72%)\n",
      "\n",
      "Train Epoch: 16 [0/5020 (0%)]\tLoss: 0.349981\n",
      "Train Epoch: 16 [400/5020 (8%)]\tLoss: 0.782498\n",
      "Train Epoch: 16 [800/5020 (16%)]\tLoss: 0.529202\n",
      "Train Epoch: 16 [1200/5020 (24%)]\tLoss: 0.963469\n",
      "Train Epoch: 16 [1600/5020 (32%)]\tLoss: 0.676052\n",
      "Train Epoch: 16 [2000/5020 (40%)]\tLoss: 0.817557\n",
      "Train Epoch: 16 [2400/5020 (48%)]\tLoss: 0.864582\n",
      "Train Epoch: 16 [2800/5020 (56%)]\tLoss: 0.040878\n",
      "Train Epoch: 16 [3200/5020 (64%)]\tLoss: 0.528414\n",
      "Train Epoch: 16 [3600/5020 (72%)]\tLoss: 0.800837\n",
      "Train Epoch: 16 [4000/5020 (80%)]\tLoss: 0.753643\n",
      "Train Epoch: 16 [4400/5020 (88%)]\tLoss: 0.669395\n",
      "Train Epoch: 16 [4800/5020 (96%)]\tLoss: 0.530630\n",
      "\n",
      "Test set: Avg. loss: 0.6017, Accuracy: 379/519 (73%)\n",
      "\n",
      "Train Epoch: 17 [0/5020 (0%)]\tLoss: 0.831231\n",
      "Train Epoch: 17 [400/5020 (8%)]\tLoss: 0.618740\n",
      "Train Epoch: 17 [800/5020 (16%)]\tLoss: 1.217644\n",
      "Train Epoch: 17 [1200/5020 (24%)]\tLoss: 0.652594\n",
      "Train Epoch: 17 [1600/5020 (32%)]\tLoss: 0.563024\n",
      "Train Epoch: 17 [2000/5020 (40%)]\tLoss: 0.421582\n",
      "Train Epoch: 17 [2400/5020 (48%)]\tLoss: 0.798327\n",
      "Train Epoch: 17 [2800/5020 (56%)]\tLoss: 0.290820\n",
      "Train Epoch: 17 [3200/5020 (64%)]\tLoss: 0.593648\n",
      "Train Epoch: 17 [3600/5020 (72%)]\tLoss: 0.531420\n",
      "Train Epoch: 17 [4000/5020 (80%)]\tLoss: 0.518922\n",
      "Train Epoch: 17 [4400/5020 (88%)]\tLoss: 0.771137\n",
      "Train Epoch: 17 [4800/5020 (96%)]\tLoss: 0.825382\n",
      "\n",
      "Test set: Avg. loss: 0.5940, Accuracy: 392/519 (76%)\n",
      "\n",
      "Train Epoch: 18 [0/5020 (0%)]\tLoss: 0.697260\n",
      "Train Epoch: 18 [400/5020 (8%)]\tLoss: 1.021939\n",
      "Train Epoch: 18 [800/5020 (16%)]\tLoss: 0.619195\n",
      "Train Epoch: 18 [1200/5020 (24%)]\tLoss: 0.773493\n",
      "Train Epoch: 18 [1600/5020 (32%)]\tLoss: 0.782485\n",
      "Train Epoch: 18 [2000/5020 (40%)]\tLoss: 0.717727\n",
      "Train Epoch: 18 [2400/5020 (48%)]\tLoss: 0.942736\n",
      "Train Epoch: 18 [2800/5020 (56%)]\tLoss: 0.000328\n",
      "Train Epoch: 18 [3200/5020 (64%)]\tLoss: 0.176674\n",
      "Train Epoch: 18 [3600/5020 (72%)]\tLoss: 0.807087\n",
      "Train Epoch: 18 [4000/5020 (80%)]\tLoss: 0.829506\n",
      "Train Epoch: 18 [4400/5020 (88%)]\tLoss: 1.403843\n",
      "Train Epoch: 18 [4800/5020 (96%)]\tLoss: 0.442325\n",
      "\n",
      "Test set: Avg. loss: 0.5857, Accuracy: 407/519 (78%)\n",
      "\n",
      "Train Epoch: 19 [0/5020 (0%)]\tLoss: 0.453195\n",
      "Train Epoch: 19 [400/5020 (8%)]\tLoss: 0.481441\n",
      "Train Epoch: 19 [800/5020 (16%)]\tLoss: 0.538469\n",
      "Train Epoch: 19 [1200/5020 (24%)]\tLoss: 0.423994\n",
      "Train Epoch: 19 [1600/5020 (32%)]\tLoss: 1.049205\n",
      "Train Epoch: 19 [2000/5020 (40%)]\tLoss: 0.709774\n",
      "Train Epoch: 19 [2400/5020 (48%)]\tLoss: 0.643134\n",
      "Train Epoch: 19 [2800/5020 (56%)]\tLoss: 1.048943\n",
      "Train Epoch: 19 [3200/5020 (64%)]\tLoss: 0.664374\n",
      "Train Epoch: 19 [3600/5020 (72%)]\tLoss: 1.390382\n",
      "Train Epoch: 19 [4000/5020 (80%)]\tLoss: 0.840300\n",
      "Train Epoch: 19 [4400/5020 (88%)]\tLoss: 0.232877\n",
      "Train Epoch: 19 [4800/5020 (96%)]\tLoss: 0.438213\n",
      "\n",
      "Test set: Avg. loss: 0.5753, Accuracy: 409/519 (79%)\n",
      "\n",
      "Train Epoch: 20 [0/5020 (0%)]\tLoss: 0.701193\n",
      "Train Epoch: 20 [400/5020 (8%)]\tLoss: 0.670203\n",
      "Train Epoch: 20 [800/5020 (16%)]\tLoss: 0.870514\n",
      "Train Epoch: 20 [1200/5020 (24%)]\tLoss: 0.306489\n",
      "Train Epoch: 20 [1600/5020 (32%)]\tLoss: 0.630082\n",
      "Train Epoch: 20 [2000/5020 (40%)]\tLoss: 0.461795\n",
      "Train Epoch: 20 [2400/5020 (48%)]\tLoss: 0.411861\n",
      "Train Epoch: 20 [2800/5020 (56%)]\tLoss: 0.382419\n",
      "Train Epoch: 20 [3200/5020 (64%)]\tLoss: 0.166820\n",
      "Train Epoch: 20 [3600/5020 (72%)]\tLoss: 0.709610\n",
      "Train Epoch: 20 [4000/5020 (80%)]\tLoss: 0.351088\n",
      "Train Epoch: 20 [4400/5020 (88%)]\tLoss: 0.511788\n",
      "Train Epoch: 20 [4800/5020 (96%)]\tLoss: 0.375391\n",
      "\n",
      "Test set: Avg. loss: 0.5654, Accuracy: 416/519 (80%)\n",
      "\n",
      "Train Epoch: 21 [0/5020 (0%)]\tLoss: 0.604792\n",
      "Train Epoch: 21 [400/5020 (8%)]\tLoss: 0.341861\n",
      "Train Epoch: 21 [800/5020 (16%)]\tLoss: 0.331564\n",
      "Train Epoch: 21 [1200/5020 (24%)]\tLoss: 0.380569\n",
      "Train Epoch: 21 [1600/5020 (32%)]\tLoss: 0.464351\n",
      "Train Epoch: 21 [2000/5020 (40%)]\tLoss: 1.058287\n",
      "Train Epoch: 21 [2400/5020 (48%)]\tLoss: 0.535302\n",
      "Train Epoch: 21 [2800/5020 (56%)]\tLoss: 4.056841\n",
      "Train Epoch: 21 [3200/5020 (64%)]\tLoss: 0.706115\n",
      "Train Epoch: 21 [3600/5020 (72%)]\tLoss: 0.007316\n",
      "Train Epoch: 21 [4000/5020 (80%)]\tLoss: 0.233206\n",
      "Train Epoch: 21 [4400/5020 (88%)]\tLoss: 0.816128\n",
      "Train Epoch: 21 [4800/5020 (96%)]\tLoss: 0.814187\n",
      "\n",
      "Test set: Avg. loss: 0.5573, Accuracy: 415/519 (80%)\n",
      "\n",
      "Train Epoch: 22 [0/5020 (0%)]\tLoss: 0.594520\n",
      "Train Epoch: 22 [400/5020 (8%)]\tLoss: 0.891216\n",
      "Train Epoch: 22 [800/5020 (16%)]\tLoss: 0.656321\n",
      "Train Epoch: 22 [1200/5020 (24%)]\tLoss: 1.146289\n",
      "Train Epoch: 22 [1600/5020 (32%)]\tLoss: 0.186709\n",
      "Train Epoch: 22 [2000/5020 (40%)]\tLoss: 0.671981\n",
      "Train Epoch: 22 [2400/5020 (48%)]\tLoss: 1.026834\n",
      "Train Epoch: 22 [2800/5020 (56%)]\tLoss: 0.840780\n",
      "Train Epoch: 22 [3200/5020 (64%)]\tLoss: 0.314425\n",
      "Train Epoch: 22 [3600/5020 (72%)]\tLoss: 0.341208\n",
      "Train Epoch: 22 [4000/5020 (80%)]\tLoss: 0.479610\n",
      "Train Epoch: 22 [4400/5020 (88%)]\tLoss: 0.512604\n",
      "Train Epoch: 22 [4800/5020 (96%)]\tLoss: 0.319604\n",
      "\n",
      "Test set: Avg. loss: 0.5619, Accuracy: 409/519 (79%)\n",
      "\n",
      "Train Epoch: 23 [0/5020 (0%)]\tLoss: 0.705184\n",
      "Train Epoch: 23 [400/5020 (8%)]\tLoss: 1.019046\n",
      "Train Epoch: 23 [800/5020 (16%)]\tLoss: 0.396922\n",
      "Train Epoch: 23 [1200/5020 (24%)]\tLoss: 0.516967\n",
      "Train Epoch: 23 [1600/5020 (32%)]\tLoss: 0.114187\n",
      "Train Epoch: 23 [2000/5020 (40%)]\tLoss: 1.143191\n",
      "Train Epoch: 23 [2400/5020 (48%)]\tLoss: 0.316687\n",
      "Train Epoch: 23 [2800/5020 (56%)]\tLoss: 0.691337\n",
      "Train Epoch: 23 [3200/5020 (64%)]\tLoss: 0.617894\n",
      "Train Epoch: 23 [3600/5020 (72%)]\tLoss: 0.734862\n",
      "Train Epoch: 23 [4000/5020 (80%)]\tLoss: 1.093676\n",
      "Train Epoch: 23 [4400/5020 (88%)]\tLoss: 0.876530\n",
      "Train Epoch: 23 [4800/5020 (96%)]\tLoss: 0.986167\n",
      "\n",
      "Test set: Avg. loss: 0.5486, Accuracy: 415/519 (80%)\n",
      "\n",
      "Train Epoch: 24 [0/5020 (0%)]\tLoss: 0.681995\n",
      "Train Epoch: 24 [400/5020 (8%)]\tLoss: 0.483421\n",
      "Train Epoch: 24 [800/5020 (16%)]\tLoss: 0.718913\n",
      "Train Epoch: 24 [1200/5020 (24%)]\tLoss: 0.676849\n",
      "Train Epoch: 24 [1600/5020 (32%)]\tLoss: 0.650779\n",
      "Train Epoch: 24 [2000/5020 (40%)]\tLoss: 0.472051\n",
      "Train Epoch: 24 [2400/5020 (48%)]\tLoss: 0.761005\n",
      "Train Epoch: 24 [2800/5020 (56%)]\tLoss: 0.584727\n",
      "Train Epoch: 24 [3200/5020 (64%)]\tLoss: 0.425165\n",
      "Train Epoch: 24 [3600/5020 (72%)]\tLoss: 0.434525\n",
      "Train Epoch: 24 [4000/5020 (80%)]\tLoss: 1.749842\n",
      "Train Epoch: 24 [4400/5020 (88%)]\tLoss: 0.485994\n",
      "Train Epoch: 24 [4800/5020 (96%)]\tLoss: 0.913867\n",
      "\n",
      "Test set: Avg. loss: 0.5333, Accuracy: 415/519 (80%)\n",
      "\n",
      "Train Epoch: 25 [0/5020 (0%)]\tLoss: 0.917194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25 [400/5020 (8%)]\tLoss: 0.861475\n",
      "Train Epoch: 25 [800/5020 (16%)]\tLoss: 0.580936\n",
      "Train Epoch: 25 [1200/5020 (24%)]\tLoss: 0.715002\n",
      "Train Epoch: 25 [1600/5020 (32%)]\tLoss: 0.970273\n",
      "Train Epoch: 25 [2000/5020 (40%)]\tLoss: 0.498904\n",
      "Train Epoch: 25 [2400/5020 (48%)]\tLoss: 0.826379\n",
      "Train Epoch: 25 [2800/5020 (56%)]\tLoss: 0.778156\n",
      "Train Epoch: 25 [3200/5020 (64%)]\tLoss: 0.615328\n",
      "Train Epoch: 25 [3600/5020 (72%)]\tLoss: 0.162532\n",
      "Train Epoch: 25 [4000/5020 (80%)]\tLoss: 0.548541\n",
      "Train Epoch: 25 [4400/5020 (88%)]\tLoss: 0.244947\n",
      "Train Epoch: 25 [4800/5020 (96%)]\tLoss: 1.333489\n",
      "\n",
      "Test set: Avg. loss: 0.5253, Accuracy: 414/519 (80%)\n",
      "\n",
      "Train Epoch: 26 [0/5020 (0%)]\tLoss: 0.599109\n",
      "Train Epoch: 26 [400/5020 (8%)]\tLoss: 0.442289\n",
      "Train Epoch: 26 [800/5020 (16%)]\tLoss: 0.306682\n",
      "Train Epoch: 26 [1200/5020 (24%)]\tLoss: 0.116286\n",
      "Train Epoch: 26 [1600/5020 (32%)]\tLoss: 0.795581\n",
      "Train Epoch: 26 [2000/5020 (40%)]\tLoss: 0.665921\n",
      "Train Epoch: 26 [2400/5020 (48%)]\tLoss: 1.172656\n",
      "Train Epoch: 26 [2800/5020 (56%)]\tLoss: 0.264448\n",
      "Train Epoch: 26 [3200/5020 (64%)]\tLoss: 0.614976\n",
      "Train Epoch: 26 [3600/5020 (72%)]\tLoss: 0.526851\n",
      "Train Epoch: 26 [4000/5020 (80%)]\tLoss: 0.187380\n",
      "Train Epoch: 26 [4400/5020 (88%)]\tLoss: 0.181059\n",
      "Train Epoch: 26 [4800/5020 (96%)]\tLoss: 0.672416\n",
      "\n",
      "Test set: Avg. loss: 0.5200, Accuracy: 417/519 (80%)\n",
      "\n",
      "Train Epoch: 27 [0/5020 (0%)]\tLoss: 0.420504\n",
      "Train Epoch: 27 [400/5020 (8%)]\tLoss: 0.609440\n",
      "Train Epoch: 27 [800/5020 (16%)]\tLoss: 1.194645\n",
      "Train Epoch: 27 [1200/5020 (24%)]\tLoss: 0.320843\n",
      "Train Epoch: 27 [1600/5020 (32%)]\tLoss: 0.659442\n",
      "Train Epoch: 27 [2000/5020 (40%)]\tLoss: 0.627284\n",
      "Train Epoch: 27 [2400/5020 (48%)]\tLoss: 0.753408\n",
      "Train Epoch: 27 [2800/5020 (56%)]\tLoss: 0.568237\n",
      "Train Epoch: 27 [3200/5020 (64%)]\tLoss: 0.565208\n",
      "Train Epoch: 27 [3600/5020 (72%)]\tLoss: 0.263663\n",
      "Train Epoch: 27 [4000/5020 (80%)]\tLoss: 0.406668\n",
      "Train Epoch: 27 [4400/5020 (88%)]\tLoss: 1.348548\n",
      "Train Epoch: 27 [4800/5020 (96%)]\tLoss: 0.595233\n",
      "\n",
      "Test set: Avg. loss: 0.5095, Accuracy: 414/519 (80%)\n",
      "\n",
      "Train Epoch: 28 [0/5020 (0%)]\tLoss: 0.000068\n",
      "Train Epoch: 28 [400/5020 (8%)]\tLoss: 0.776516\n",
      "Train Epoch: 28 [800/5020 (16%)]\tLoss: 0.667209\n",
      "Train Epoch: 28 [1200/5020 (24%)]\tLoss: 0.324235\n",
      "Train Epoch: 28 [1600/5020 (32%)]\tLoss: 0.768827\n",
      "Train Epoch: 28 [2000/5020 (40%)]\tLoss: 0.492404\n",
      "Train Epoch: 28 [2400/5020 (48%)]\tLoss: 0.310572\n",
      "Train Epoch: 28 [2800/5020 (56%)]\tLoss: 1.085294\n",
      "Train Epoch: 28 [3200/5020 (64%)]\tLoss: 0.322939\n",
      "Train Epoch: 28 [3600/5020 (72%)]\tLoss: 0.149699\n",
      "Train Epoch: 28 [4000/5020 (80%)]\tLoss: 0.733903\n",
      "Train Epoch: 28 [4400/5020 (88%)]\tLoss: 0.665480\n",
      "Train Epoch: 28 [4800/5020 (96%)]\tLoss: 0.651872\n",
      "\n",
      "Test set: Avg. loss: 0.5143, Accuracy: 421/519 (81%)\n",
      "\n",
      "Train Epoch: 29 [0/5020 (0%)]\tLoss: 0.851502\n",
      "Train Epoch: 29 [400/5020 (8%)]\tLoss: 0.287380\n",
      "Train Epoch: 29 [800/5020 (16%)]\tLoss: 0.592843\n",
      "Train Epoch: 29 [1200/5020 (24%)]\tLoss: 0.424046\n",
      "Train Epoch: 29 [1600/5020 (32%)]\tLoss: 0.582122\n",
      "Train Epoch: 29 [2000/5020 (40%)]\tLoss: 0.430034\n",
      "Train Epoch: 29 [2400/5020 (48%)]\tLoss: 0.766235\n",
      "Train Epoch: 29 [2800/5020 (56%)]\tLoss: 0.738656\n",
      "Train Epoch: 29 [3200/5020 (64%)]\tLoss: 0.435154\n",
      "Train Epoch: 29 [3600/5020 (72%)]\tLoss: 0.473229\n",
      "Train Epoch: 29 [4000/5020 (80%)]\tLoss: 0.757277\n",
      "Train Epoch: 29 [4400/5020 (88%)]\tLoss: 0.656361\n",
      "Train Epoch: 29 [4800/5020 (96%)]\tLoss: 0.398643\n",
      "\n",
      "Test set: Avg. loss: 0.4974, Accuracy: 413/519 (80%)\n",
      "\n",
      "Train Epoch: 30 [0/5020 (0%)]\tLoss: 0.268253\n",
      "Train Epoch: 30 [400/5020 (8%)]\tLoss: 0.142189\n",
      "Train Epoch: 30 [800/5020 (16%)]\tLoss: 0.436622\n",
      "Train Epoch: 30 [1200/5020 (24%)]\tLoss: 0.339581\n",
      "Train Epoch: 30 [1600/5020 (32%)]\tLoss: 0.443637\n",
      "Train Epoch: 30 [2000/5020 (40%)]\tLoss: 0.634019\n",
      "Train Epoch: 30 [2400/5020 (48%)]\tLoss: 0.452881\n",
      "Train Epoch: 30 [2800/5020 (56%)]\tLoss: 0.520102\n",
      "Train Epoch: 30 [3200/5020 (64%)]\tLoss: 0.664489\n",
      "Train Epoch: 30 [3600/5020 (72%)]\tLoss: 0.731074\n",
      "Train Epoch: 30 [4000/5020 (80%)]\tLoss: 0.586122\n",
      "Train Epoch: 30 [4400/5020 (88%)]\tLoss: 0.278910\n",
      "Train Epoch: 30 [4800/5020 (96%)]\tLoss: 0.309922\n",
      "\n",
      "Test set: Avg. loss: 0.4966, Accuracy: 418/519 (81%)\n",
      "\n",
      "Train Epoch: 31 [0/5020 (0%)]\tLoss: 0.575172\n",
      "Train Epoch: 31 [400/5020 (8%)]\tLoss: 0.778652\n",
      "Train Epoch: 31 [800/5020 (16%)]\tLoss: 0.476854\n",
      "Train Epoch: 31 [1200/5020 (24%)]\tLoss: 0.591197\n",
      "Train Epoch: 31 [1600/5020 (32%)]\tLoss: 0.678866\n",
      "Train Epoch: 31 [2000/5020 (40%)]\tLoss: 0.725830\n",
      "Train Epoch: 31 [2400/5020 (48%)]\tLoss: 0.703850\n",
      "Train Epoch: 31 [2800/5020 (56%)]\tLoss: 0.288690\n",
      "Train Epoch: 31 [3200/5020 (64%)]\tLoss: 0.524735\n",
      "Train Epoch: 31 [3600/5020 (72%)]\tLoss: 0.135510\n",
      "Train Epoch: 31 [4000/5020 (80%)]\tLoss: 0.264839\n",
      "Train Epoch: 31 [4400/5020 (88%)]\tLoss: 0.523931\n",
      "Train Epoch: 31 [4800/5020 (96%)]\tLoss: 1.292326\n",
      "\n",
      "Test set: Avg. loss: 0.5114, Accuracy: 421/519 (81%)\n",
      "\n",
      "Train Epoch: 32 [0/5020 (0%)]\tLoss: 0.398696\n",
      "Train Epoch: 32 [400/5020 (8%)]\tLoss: 0.249803\n",
      "Train Epoch: 32 [800/5020 (16%)]\tLoss: 0.242981\n",
      "Train Epoch: 32 [1200/5020 (24%)]\tLoss: 0.270895\n",
      "Train Epoch: 32 [1600/5020 (32%)]\tLoss: 0.407385\n",
      "Train Epoch: 32 [2000/5020 (40%)]\tLoss: 0.696045\n",
      "Train Epoch: 32 [2400/5020 (48%)]\tLoss: 0.496965\n",
      "Train Epoch: 32 [2800/5020 (56%)]\tLoss: 0.261758\n",
      "Train Epoch: 32 [3200/5020 (64%)]\tLoss: 0.495272\n",
      "Train Epoch: 32 [3600/5020 (72%)]\tLoss: 1.221764\n",
      "Train Epoch: 32 [4000/5020 (80%)]\tLoss: 0.126370\n",
      "Train Epoch: 32 [4400/5020 (88%)]\tLoss: 0.463989\n",
      "Train Epoch: 32 [4800/5020 (96%)]\tLoss: 0.684346\n",
      "\n",
      "Test set: Avg. loss: 0.4972, Accuracy: 422/519 (81%)\n",
      "\n",
      "Train Epoch: 33 [0/5020 (0%)]\tLoss: 0.612727\n",
      "Train Epoch: 33 [400/5020 (8%)]\tLoss: 0.747709\n",
      "Train Epoch: 33 [800/5020 (16%)]\tLoss: 0.710498\n",
      "Train Epoch: 33 [1200/5020 (24%)]\tLoss: 0.324875\n",
      "Train Epoch: 33 [1600/5020 (32%)]\tLoss: 0.736938\n",
      "Train Epoch: 33 [2000/5020 (40%)]\tLoss: 0.521910\n",
      "Train Epoch: 33 [2400/5020 (48%)]\tLoss: 0.536354\n",
      "Train Epoch: 33 [2800/5020 (56%)]\tLoss: 0.483780\n",
      "Train Epoch: 33 [3200/5020 (64%)]\tLoss: 0.252019\n",
      "Train Epoch: 33 [3600/5020 (72%)]\tLoss: 0.435710\n",
      "Train Epoch: 33 [4000/5020 (80%)]\tLoss: 0.406174\n",
      "Train Epoch: 33 [4400/5020 (88%)]\tLoss: 0.443894\n",
      "Train Epoch: 33 [4800/5020 (96%)]\tLoss: 0.250932\n",
      "\n",
      "Test set: Avg. loss: 0.4952, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 34 [0/5020 (0%)]\tLoss: 0.563593\n",
      "Train Epoch: 34 [400/5020 (8%)]\tLoss: 0.956001\n",
      "Train Epoch: 34 [800/5020 (16%)]\tLoss: 0.677978\n",
      "Train Epoch: 34 [1200/5020 (24%)]\tLoss: 0.466072\n",
      "Train Epoch: 34 [1600/5020 (32%)]\tLoss: 0.510577\n",
      "Train Epoch: 34 [2000/5020 (40%)]\tLoss: 0.237074\n",
      "Train Epoch: 34 [2400/5020 (48%)]\tLoss: 0.325854\n",
      "Train Epoch: 34 [2800/5020 (56%)]\tLoss: 0.646783\n",
      "Train Epoch: 34 [3200/5020 (64%)]\tLoss: 0.443925\n",
      "Train Epoch: 34 [3600/5020 (72%)]\tLoss: 0.506149\n",
      "Train Epoch: 34 [4000/5020 (80%)]\tLoss: 1.206231\n",
      "Train Epoch: 34 [4400/5020 (88%)]\tLoss: 0.403085\n",
      "Train Epoch: 34 [4800/5020 (96%)]\tLoss: 0.156799\n",
      "\n",
      "Test set: Avg. loss: 0.5037, Accuracy: 421/519 (81%)\n",
      "\n",
      "Train Epoch: 35 [0/5020 (0%)]\tLoss: 0.156490\n",
      "Train Epoch: 35 [400/5020 (8%)]\tLoss: 0.585034\n",
      "Train Epoch: 35 [800/5020 (16%)]\tLoss: 0.474006\n",
      "Train Epoch: 35 [1200/5020 (24%)]\tLoss: 0.558122\n",
      "Train Epoch: 35 [1600/5020 (32%)]\tLoss: 0.291917\n",
      "Train Epoch: 35 [2000/5020 (40%)]\tLoss: 0.672008\n",
      "Train Epoch: 35 [2400/5020 (48%)]\tLoss: 1.313799\n",
      "Train Epoch: 35 [2800/5020 (56%)]\tLoss: 0.907056\n",
      "Train Epoch: 35 [3200/5020 (64%)]\tLoss: 0.317794\n",
      "Train Epoch: 35 [3600/5020 (72%)]\tLoss: 0.409245\n",
      "Train Epoch: 35 [4000/5020 (80%)]\tLoss: 1.051599\n",
      "Train Epoch: 35 [4400/5020 (88%)]\tLoss: 0.689832\n",
      "Train Epoch: 35 [4800/5020 (96%)]\tLoss: 0.069159\n",
      "\n",
      "Test set: Avg. loss: 0.4910, Accuracy: 424/519 (82%)\n",
      "\n",
      "Train Epoch: 36 [0/5020 (0%)]\tLoss: 0.646896\n",
      "Train Epoch: 36 [400/5020 (8%)]\tLoss: 0.255752\n",
      "Train Epoch: 36 [800/5020 (16%)]\tLoss: 0.576934\n",
      "Train Epoch: 36 [1200/5020 (24%)]\tLoss: 1.028232\n",
      "Train Epoch: 36 [1600/5020 (32%)]\tLoss: 0.162382\n",
      "Train Epoch: 36 [2000/5020 (40%)]\tLoss: 0.249909\n",
      "Train Epoch: 36 [2400/5020 (48%)]\tLoss: 0.767376\n",
      "Train Epoch: 36 [2800/5020 (56%)]\tLoss: 0.404257\n",
      "Train Epoch: 36 [3200/5020 (64%)]\tLoss: 0.388132\n",
      "Train Epoch: 36 [3600/5020 (72%)]\tLoss: 0.643974\n",
      "Train Epoch: 36 [4000/5020 (80%)]\tLoss: 0.605747\n",
      "Train Epoch: 36 [4400/5020 (88%)]\tLoss: 0.729676\n",
      "Train Epoch: 36 [4800/5020 (96%)]\tLoss: 0.509292\n",
      "\n",
      "Test set: Avg. loss: 0.4905, Accuracy: 426/519 (82%)\n",
      "\n",
      "Train Epoch: 37 [0/5020 (0%)]\tLoss: 0.993488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37 [400/5020 (8%)]\tLoss: 0.765176\n",
      "Train Epoch: 37 [800/5020 (16%)]\tLoss: 0.843082\n",
      "Train Epoch: 37 [1200/5020 (24%)]\tLoss: 0.400372\n",
      "Train Epoch: 37 [1600/5020 (32%)]\tLoss: 0.480521\n",
      "Train Epoch: 37 [2000/5020 (40%)]\tLoss: 7.383526\n",
      "Train Epoch: 37 [2400/5020 (48%)]\tLoss: 0.313620\n",
      "Train Epoch: 37 [2800/5020 (56%)]\tLoss: 0.257117\n",
      "Train Epoch: 37 [3200/5020 (64%)]\tLoss: 0.606934\n",
      "Train Epoch: 37 [3600/5020 (72%)]\tLoss: 0.409820\n",
      "Train Epoch: 37 [4000/5020 (80%)]\tLoss: 0.608504\n",
      "Train Epoch: 37 [4400/5020 (88%)]\tLoss: 0.337492\n",
      "Train Epoch: 37 [4800/5020 (96%)]\tLoss: 0.766120\n",
      "\n",
      "Test set: Avg. loss: 0.5047, Accuracy: 420/519 (81%)\n",
      "\n",
      "Train Epoch: 38 [0/5020 (0%)]\tLoss: 0.667535\n",
      "Train Epoch: 38 [400/5020 (8%)]\tLoss: 0.707156\n",
      "Train Epoch: 38 [800/5020 (16%)]\tLoss: 0.661562\n",
      "Train Epoch: 38 [1200/5020 (24%)]\tLoss: 0.297282\n",
      "Train Epoch: 38 [1600/5020 (32%)]\tLoss: 0.410457\n",
      "Train Epoch: 38 [2000/5020 (40%)]\tLoss: 1.102776\n",
      "Train Epoch: 38 [2400/5020 (48%)]\tLoss: 0.239747\n",
      "Train Epoch: 38 [2800/5020 (56%)]\tLoss: 0.228700\n",
      "Train Epoch: 38 [3200/5020 (64%)]\tLoss: 0.767559\n",
      "Train Epoch: 38 [3600/5020 (72%)]\tLoss: 1.182763\n",
      "Train Epoch: 38 [4000/5020 (80%)]\tLoss: 0.980787\n",
      "Train Epoch: 38 [4400/5020 (88%)]\tLoss: 0.537470\n",
      "Train Epoch: 38 [4800/5020 (96%)]\tLoss: 0.672822\n",
      "\n",
      "Test set: Avg. loss: 0.4855, Accuracy: 427/519 (82%)\n",
      "\n",
      "Train Epoch: 39 [0/5020 (0%)]\tLoss: 0.695626\n",
      "Train Epoch: 39 [400/5020 (8%)]\tLoss: 0.035256\n",
      "Train Epoch: 39 [800/5020 (16%)]\tLoss: 0.373017\n",
      "Train Epoch: 39 [1200/5020 (24%)]\tLoss: 0.128546\n",
      "Train Epoch: 39 [1600/5020 (32%)]\tLoss: 0.578136\n",
      "Train Epoch: 39 [2000/5020 (40%)]\tLoss: 0.466324\n",
      "Train Epoch: 39 [2400/5020 (48%)]\tLoss: 0.705814\n",
      "Train Epoch: 39 [2800/5020 (56%)]\tLoss: 0.352641\n",
      "Train Epoch: 39 [3200/5020 (64%)]\tLoss: 1.428908\n",
      "Train Epoch: 39 [3600/5020 (72%)]\tLoss: 0.615189\n",
      "Train Epoch: 39 [4000/5020 (80%)]\tLoss: 0.298302\n",
      "Train Epoch: 39 [4400/5020 (88%)]\tLoss: 0.355228\n",
      "Train Epoch: 39 [4800/5020 (96%)]\tLoss: 0.287522\n",
      "\n",
      "Test set: Avg. loss: 0.4819, Accuracy: 423/519 (82%)\n",
      "\n",
      "Train Epoch: 40 [0/5020 (0%)]\tLoss: 1.061560\n",
      "Train Epoch: 40 [400/5020 (8%)]\tLoss: 1.215379\n",
      "Train Epoch: 40 [800/5020 (16%)]\tLoss: 0.727334\n",
      "Train Epoch: 40 [1200/5020 (24%)]\tLoss: 0.386790\n",
      "Train Epoch: 40 [1600/5020 (32%)]\tLoss: 0.389733\n",
      "Train Epoch: 40 [2000/5020 (40%)]\tLoss: 0.411351\n",
      "Train Epoch: 40 [2400/5020 (48%)]\tLoss: 0.176472\n",
      "Train Epoch: 40 [2800/5020 (56%)]\tLoss: 0.710847\n",
      "Train Epoch: 40 [3200/5020 (64%)]\tLoss: 2.980781\n",
      "Train Epoch: 40 [3600/5020 (72%)]\tLoss: 0.053807\n",
      "Train Epoch: 40 [4000/5020 (80%)]\tLoss: 0.127450\n",
      "Train Epoch: 40 [4400/5020 (88%)]\tLoss: 0.119754\n",
      "Train Epoch: 40 [4800/5020 (96%)]\tLoss: 0.377102\n",
      "\n",
      "Test set: Avg. loss: 0.4872, Accuracy: 421/519 (81%)\n",
      "\n",
      "Train Epoch: 41 [0/5020 (0%)]\tLoss: 0.592076\n",
      "Train Epoch: 41 [400/5020 (8%)]\tLoss: 0.596809\n",
      "Train Epoch: 41 [800/5020 (16%)]\tLoss: 0.360405\n",
      "Train Epoch: 41 [1200/5020 (24%)]\tLoss: 0.574363\n",
      "Train Epoch: 41 [1600/5020 (32%)]\tLoss: 0.736056\n",
      "Train Epoch: 41 [2000/5020 (40%)]\tLoss: 0.032571\n",
      "Train Epoch: 41 [2400/5020 (48%)]\tLoss: 0.785029\n",
      "Train Epoch: 41 [2800/5020 (56%)]\tLoss: 0.751614\n",
      "Train Epoch: 41 [3200/5020 (64%)]\tLoss: 1.115710\n",
      "Train Epoch: 41 [3600/5020 (72%)]\tLoss: 0.249384\n",
      "Train Epoch: 41 [4000/5020 (80%)]\tLoss: 0.787281\n",
      "Train Epoch: 41 [4400/5020 (88%)]\tLoss: 0.722816\n",
      "Train Epoch: 41 [4800/5020 (96%)]\tLoss: 0.613844\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "Test set: Avg. loss: 0.4800, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 42 [0/5020 (0%)]\tLoss: 1.163916\n",
      "Train Epoch: 42 [400/5020 (8%)]\tLoss: 0.524218\n",
      "Train Epoch: 42 [800/5020 (16%)]\tLoss: 0.452566\n",
      "Train Epoch: 42 [1200/5020 (24%)]\tLoss: 1.037912\n",
      "Train Epoch: 42 [1600/5020 (32%)]\tLoss: 0.507579\n",
      "Train Epoch: 42 [2000/5020 (40%)]\tLoss: 0.788360\n",
      "Train Epoch: 42 [2400/5020 (48%)]\tLoss: 0.720883\n",
      "Train Epoch: 42 [2800/5020 (56%)]\tLoss: 0.468758\n",
      "Train Epoch: 42 [3200/5020 (64%)]\tLoss: 0.678613\n",
      "Train Epoch: 42 [3600/5020 (72%)]\tLoss: 0.555562\n",
      "Train Epoch: 42 [4000/5020 (80%)]\tLoss: 0.764549\n",
      "Train Epoch: 42 [4400/5020 (88%)]\tLoss: 0.416474\n",
      "Train Epoch: 42 [4800/5020 (96%)]\tLoss: 0.482884\n",
      "\n",
      "Test set: Avg. loss: 0.4762, Accuracy: 423/519 (82%)\n",
      "\n",
      "Train Epoch: 43 [0/5020 (0%)]\tLoss: 0.348312\n",
      "Train Epoch: 43 [400/5020 (8%)]\tLoss: 0.287767\n",
      "Train Epoch: 43 [800/5020 (16%)]\tLoss: 0.851317\n",
      "Train Epoch: 43 [1200/5020 (24%)]\tLoss: 0.543520\n",
      "Train Epoch: 43 [1600/5020 (32%)]\tLoss: 0.929752\n",
      "Train Epoch: 43 [2000/5020 (40%)]\tLoss: 0.243538\n",
      "Train Epoch: 43 [2400/5020 (48%)]\tLoss: 0.763462\n",
      "Train Epoch: 43 [2800/5020 (56%)]\tLoss: 1.269199\n",
      "Train Epoch: 43 [3200/5020 (64%)]\tLoss: 0.588266\n",
      "Train Epoch: 43 [3600/5020 (72%)]\tLoss: 0.704043\n",
      "Train Epoch: 43 [4000/5020 (80%)]\tLoss: 0.527671\n",
      "Train Epoch: 43 [4400/5020 (88%)]\tLoss: 0.405022\n",
      "Train Epoch: 43 [4800/5020 (96%)]\tLoss: 0.700115\n",
      "\n",
      "Test set: Avg. loss: 0.4734, Accuracy: 420/519 (81%)\n",
      "\n",
      "Train Epoch: 44 [0/5020 (0%)]\tLoss: 0.522922\n",
      "Train Epoch: 44 [400/5020 (8%)]\tLoss: 0.457448\n",
      "Train Epoch: 44 [800/5020 (16%)]\tLoss: 0.673708\n",
      "Train Epoch: 44 [1200/5020 (24%)]\tLoss: 0.673306\n",
      "Train Epoch: 44 [1600/5020 (32%)]\tLoss: 0.758441\n",
      "Train Epoch: 44 [2000/5020 (40%)]\tLoss: 0.303462\n",
      "Train Epoch: 44 [2400/5020 (48%)]\tLoss: 0.636742\n",
      "Train Epoch: 44 [2800/5020 (56%)]\tLoss: 0.181879\n",
      "Train Epoch: 44 [3200/5020 (64%)]\tLoss: 0.576109\n",
      "Train Epoch: 44 [3600/5020 (72%)]\tLoss: 0.378506\n",
      "Train Epoch: 44 [4000/5020 (80%)]\tLoss: 0.306498\n",
      "Train Epoch: 44 [4400/5020 (88%)]\tLoss: 0.468059\n",
      "Train Epoch: 44 [4800/5020 (96%)]\tLoss: 0.792578\n",
      "\n",
      "Test set: Avg. loss: 0.4783, Accuracy: 423/519 (82%)\n",
      "\n",
      "Train Epoch: 45 [0/5020 (0%)]\tLoss: 0.282262\n",
      "Train Epoch: 45 [400/5020 (8%)]\tLoss: 0.966394\n",
      "Train Epoch: 45 [800/5020 (16%)]\tLoss: 0.301782\n",
      "Train Epoch: 45 [1200/5020 (24%)]\tLoss: 0.990238\n",
      "Train Epoch: 45 [1600/5020 (32%)]\tLoss: 0.268545\n",
      "Train Epoch: 45 [2000/5020 (40%)]\tLoss: 0.041011\n",
      "Train Epoch: 45 [2400/5020 (48%)]\tLoss: 0.976933\n",
      "Train Epoch: 45 [2800/5020 (56%)]\tLoss: 0.455909\n",
      "Train Epoch: 45 [3200/5020 (64%)]\tLoss: 0.310447\n",
      "Train Epoch: 45 [3600/5020 (72%)]\tLoss: 0.482559\n",
      "Train Epoch: 45 [4000/5020 (80%)]\tLoss: 0.009845\n",
      "Train Epoch: 45 [4400/5020 (88%)]\tLoss: 0.232618\n",
      "Train Epoch: 45 [4800/5020 (96%)]\tLoss: 0.354542\n",
      "\n",
      "Test set: Avg. loss: 0.4781, Accuracy: 423/519 (82%)\n",
      "\n",
      "Train Epoch: 46 [0/5020 (0%)]\tLoss: 0.241361\n",
      "Train Epoch: 46 [400/5020 (8%)]\tLoss: 0.519929\n",
      "Train Epoch: 46 [800/5020 (16%)]\tLoss: 0.232579\n",
      "Train Epoch: 46 [1200/5020 (24%)]\tLoss: 0.596434\n",
      "Train Epoch: 46 [1600/5020 (32%)]\tLoss: 0.685608\n",
      "Train Epoch: 46 [2000/5020 (40%)]\tLoss: 0.566102\n",
      "Train Epoch: 46 [2400/5020 (48%)]\tLoss: 0.560349\n",
      "Train Epoch: 46 [2800/5020 (56%)]\tLoss: 0.584217\n",
      "Train Epoch: 46 [3200/5020 (64%)]\tLoss: 0.416080\n",
      "Train Epoch: 46 [3600/5020 (72%)]\tLoss: 0.782332\n",
      "Train Epoch: 46 [4000/5020 (80%)]\tLoss: 0.114375\n",
      "Train Epoch: 46 [4400/5020 (88%)]\tLoss: 0.358909\n",
      "Train Epoch: 46 [4800/5020 (96%)]\tLoss: 0.238003\n",
      "\n",
      "Test set: Avg. loss: 0.4770, Accuracy: 423/519 (82%)\n",
      "\n",
      "Train Epoch: 47 [0/5020 (0%)]\tLoss: 0.227300\n",
      "Train Epoch: 47 [400/5020 (8%)]\tLoss: 1.198965\n",
      "Train Epoch: 47 [800/5020 (16%)]\tLoss: 0.559424\n",
      "Train Epoch: 47 [1200/5020 (24%)]\tLoss: 0.586563\n",
      "Train Epoch: 47 [1600/5020 (32%)]\tLoss: 0.716793\n",
      "Train Epoch: 47 [2000/5020 (40%)]\tLoss: 1.172693\n",
      "Train Epoch: 47 [2400/5020 (48%)]\tLoss: 0.379208\n",
      "Train Epoch: 47 [2800/5020 (56%)]\tLoss: 0.344540\n",
      "Train Epoch: 47 [3200/5020 (64%)]\tLoss: 0.275878\n",
      "Train Epoch: 47 [3600/5020 (72%)]\tLoss: 1.196490\n",
      "Train Epoch: 47 [4000/5020 (80%)]\tLoss: 0.266174\n",
      "Train Epoch: 47 [4400/5020 (88%)]\tLoss: 0.290788\n",
      "Train Epoch: 47 [4800/5020 (96%)]\tLoss: 0.323663\n",
      "\n",
      "Test set: Avg. loss: 0.4760, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 48 [0/5020 (0%)]\tLoss: 0.123649\n",
      "Train Epoch: 48 [400/5020 (8%)]\tLoss: 0.530217\n",
      "Train Epoch: 48 [800/5020 (16%)]\tLoss: 0.003940\n",
      "Train Epoch: 48 [1200/5020 (24%)]\tLoss: 0.271104\n",
      "Train Epoch: 48 [1600/5020 (32%)]\tLoss: 0.588967\n",
      "Train Epoch: 48 [2000/5020 (40%)]\tLoss: 0.469872\n",
      "Train Epoch: 48 [2400/5020 (48%)]\tLoss: 0.572836\n",
      "Train Epoch: 48 [2800/5020 (56%)]\tLoss: 0.235965\n",
      "Train Epoch: 48 [3200/5020 (64%)]\tLoss: 0.498917\n",
      "Train Epoch: 48 [3600/5020 (72%)]\tLoss: 0.616560\n",
      "Train Epoch: 48 [4000/5020 (80%)]\tLoss: 0.492777\n",
      "Train Epoch: 48 [4400/5020 (88%)]\tLoss: 0.446396\n",
      "Train Epoch: 48 [4800/5020 (96%)]\tLoss: 0.689452\n",
      "\n",
      "Test set: Avg. loss: 0.4755, Accuracy: 425/519 (82%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [0/5020 (0%)]\tLoss: 0.576121\n",
      "Train Epoch: 49 [400/5020 (8%)]\tLoss: 0.468227\n",
      "Train Epoch: 49 [800/5020 (16%)]\tLoss: 0.533679\n",
      "Train Epoch: 49 [1200/5020 (24%)]\tLoss: 0.756001\n",
      "Train Epoch: 49 [1600/5020 (32%)]\tLoss: 0.248438\n",
      "Train Epoch: 49 [2000/5020 (40%)]\tLoss: 0.681468\n",
      "Train Epoch: 49 [2400/5020 (48%)]\tLoss: 0.317794\n",
      "Train Epoch: 49 [2800/5020 (56%)]\tLoss: 0.423397\n",
      "Train Epoch: 49 [3200/5020 (64%)]\tLoss: 0.826671\n",
      "Train Epoch: 49 [3600/5020 (72%)]\tLoss: 0.656341\n",
      "Train Epoch: 49 [4000/5020 (80%)]\tLoss: 0.311322\n",
      "Train Epoch: 49 [4400/5020 (88%)]\tLoss: 0.445820\n",
      "Train Epoch: 49 [4800/5020 (96%)]\tLoss: 0.491109\n",
      "\n",
      "Test set: Avg. loss: 0.4757, Accuracy: 423/519 (82%)\n",
      "\n",
      "Train Epoch: 50 [0/5020 (0%)]\tLoss: 0.240638\n",
      "Train Epoch: 50 [400/5020 (8%)]\tLoss: 0.620545\n",
      "Train Epoch: 50 [800/5020 (16%)]\tLoss: 0.282577\n",
      "Train Epoch: 50 [1200/5020 (24%)]\tLoss: 1.366042\n",
      "Train Epoch: 50 [1600/5020 (32%)]\tLoss: 1.613927\n",
      "Train Epoch: 50 [2000/5020 (40%)]\tLoss: 0.657407\n",
      "Train Epoch: 50 [2400/5020 (48%)]\tLoss: 0.006006\n",
      "Train Epoch: 50 [2800/5020 (56%)]\tLoss: 0.379909\n",
      "Train Epoch: 50 [3200/5020 (64%)]\tLoss: 0.438291\n",
      "Train Epoch: 50 [3600/5020 (72%)]\tLoss: 0.703738\n",
      "Train Epoch: 50 [4000/5020 (80%)]\tLoss: 1.081517\n",
      "Train Epoch: 50 [4400/5020 (88%)]\tLoss: 0.491791\n",
      "Train Epoch: 50 [4800/5020 (96%)]\tLoss: 0.926789\n",
      "\n",
      "Test set: Avg. loss: 0.4764, Accuracy: 423/519 (82%)\n",
      "\n",
      "Train Epoch: 51 [0/5020 (0%)]\tLoss: 0.504119\n",
      "Train Epoch: 51 [400/5020 (8%)]\tLoss: 0.245993\n",
      "Train Epoch: 51 [800/5020 (16%)]\tLoss: 0.237167\n",
      "Train Epoch: 51 [1200/5020 (24%)]\tLoss: 0.450883\n",
      "Train Epoch: 51 [1600/5020 (32%)]\tLoss: 0.230879\n",
      "Train Epoch: 51 [2000/5020 (40%)]\tLoss: 0.630964\n",
      "Train Epoch: 51 [2400/5020 (48%)]\tLoss: 0.662203\n",
      "Train Epoch: 51 [2800/5020 (56%)]\tLoss: 0.429559\n",
      "Train Epoch: 51 [3200/5020 (64%)]\tLoss: 0.869607\n",
      "Train Epoch: 51 [3600/5020 (72%)]\tLoss: 0.270216\n",
      "Train Epoch: 51 [4000/5020 (80%)]\tLoss: 0.813479\n",
      "Train Epoch: 51 [4400/5020 (88%)]\tLoss: 0.266266\n",
      "Train Epoch: 51 [4800/5020 (96%)]\tLoss: 0.838953\n",
      "\n",
      "Test set: Avg. loss: 0.4759, Accuracy: 424/519 (82%)\n",
      "\n",
      "Train Epoch: 52 [0/5020 (0%)]\tLoss: 1.382845\n",
      "Train Epoch: 52 [400/5020 (8%)]\tLoss: 0.741360\n",
      "Train Epoch: 52 [800/5020 (16%)]\tLoss: 0.627579\n",
      "Train Epoch: 52 [1200/5020 (24%)]\tLoss: 0.045058\n",
      "Train Epoch: 52 [1600/5020 (32%)]\tLoss: 0.467207\n",
      "Train Epoch: 52 [2000/5020 (40%)]\tLoss: 1.003071\n",
      "Train Epoch: 52 [2400/5020 (48%)]\tLoss: 0.350152\n",
      "Train Epoch: 52 [2800/5020 (56%)]\tLoss: 0.412247\n",
      "Train Epoch: 52 [3200/5020 (64%)]\tLoss: 0.240240\n",
      "Train Epoch: 52 [3600/5020 (72%)]\tLoss: 0.237170\n",
      "Train Epoch: 52 [4000/5020 (80%)]\tLoss: 0.110088\n",
      "Train Epoch: 52 [4400/5020 (88%)]\tLoss: 0.346975\n",
      "Train Epoch: 52 [4800/5020 (96%)]\tLoss: 0.491164\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "\n",
      "Test set: Avg. loss: 0.4737, Accuracy: 426/519 (82%)\n",
      "\n",
      "Train Epoch: 53 [0/5020 (0%)]\tLoss: 0.425898\n",
      "Train Epoch: 53 [400/5020 (8%)]\tLoss: 1.974601\n",
      "Train Epoch: 53 [800/5020 (16%)]\tLoss: 0.163535\n",
      "Train Epoch: 53 [1200/5020 (24%)]\tLoss: 0.519938\n",
      "Train Epoch: 53 [1600/5020 (32%)]\tLoss: 0.399091\n",
      "Train Epoch: 53 [2000/5020 (40%)]\tLoss: 0.842194\n",
      "Train Epoch: 53 [2400/5020 (48%)]\tLoss: 0.179484\n",
      "Train Epoch: 53 [2800/5020 (56%)]\tLoss: 0.136549\n",
      "Train Epoch: 53 [3200/5020 (64%)]\tLoss: 0.640592\n",
      "Train Epoch: 53 [3600/5020 (72%)]\tLoss: 0.937936\n",
      "Train Epoch: 53 [4000/5020 (80%)]\tLoss: 0.360235\n",
      "Train Epoch: 53 [4400/5020 (88%)]\tLoss: 1.962662\n",
      "Train Epoch: 53 [4800/5020 (96%)]\tLoss: 1.182246\n",
      "\n",
      "Test set: Avg. loss: 0.4736, Accuracy: 426/519 (82%)\n",
      "\n",
      "Train Epoch: 54 [0/5020 (0%)]\tLoss: 0.618625\n",
      "Train Epoch: 54 [400/5020 (8%)]\tLoss: 0.539512\n",
      "Train Epoch: 54 [800/5020 (16%)]\tLoss: 0.245321\n",
      "Train Epoch: 54 [1200/5020 (24%)]\tLoss: 0.263844\n",
      "Train Epoch: 54 [1600/5020 (32%)]\tLoss: 1.027485\n",
      "Train Epoch: 54 [2000/5020 (40%)]\tLoss: 0.585333\n",
      "Train Epoch: 54 [2400/5020 (48%)]\tLoss: 0.806931\n",
      "Train Epoch: 54 [2800/5020 (56%)]\tLoss: 0.761452\n",
      "Train Epoch: 54 [3200/5020 (64%)]\tLoss: 0.652138\n",
      "Train Epoch: 54 [3600/5020 (72%)]\tLoss: 1.199007\n",
      "Train Epoch: 54 [4000/5020 (80%)]\tLoss: 0.407141\n",
      "Train Epoch: 54 [4400/5020 (88%)]\tLoss: 0.727833\n",
      "Train Epoch: 54 [4800/5020 (96%)]\tLoss: 0.421304\n",
      "\n",
      "Test set: Avg. loss: 0.4740, Accuracy: 426/519 (82%)\n",
      "\n",
      "Train Epoch: 55 [0/5020 (0%)]\tLoss: 0.373175\n",
      "Train Epoch: 55 [400/5020 (8%)]\tLoss: 0.251034\n",
      "Train Epoch: 55 [800/5020 (16%)]\tLoss: 0.740270\n",
      "Train Epoch: 55 [1200/5020 (24%)]\tLoss: 0.369207\n",
      "Train Epoch: 55 [1600/5020 (32%)]\tLoss: 0.233652\n",
      "Train Epoch: 55 [2000/5020 (40%)]\tLoss: 0.779760\n",
      "Train Epoch: 55 [2400/5020 (48%)]\tLoss: 0.431245\n",
      "Train Epoch: 55 [2800/5020 (56%)]\tLoss: 0.341198\n",
      "Train Epoch: 55 [3200/5020 (64%)]\tLoss: 0.456955\n",
      "Train Epoch: 55 [3600/5020 (72%)]\tLoss: 0.958035\n",
      "Train Epoch: 55 [4000/5020 (80%)]\tLoss: 0.831893\n",
      "Train Epoch: 55 [4400/5020 (88%)]\tLoss: 1.299420\n",
      "Train Epoch: 55 [4800/5020 (96%)]\tLoss: 0.551689\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 424/519 (82%)\n",
      "\n",
      "Train Epoch: 56 [0/5020 (0%)]\tLoss: 0.303966\n",
      "Train Epoch: 56 [400/5020 (8%)]\tLoss: 0.226448\n",
      "Train Epoch: 56 [800/5020 (16%)]\tLoss: 0.781572\n",
      "Train Epoch: 56 [1200/5020 (24%)]\tLoss: 0.365717\n",
      "Train Epoch: 56 [1600/5020 (32%)]\tLoss: 0.328608\n",
      "Train Epoch: 56 [2000/5020 (40%)]\tLoss: 0.666072\n",
      "Train Epoch: 56 [2400/5020 (48%)]\tLoss: 0.448155\n",
      "Train Epoch: 56 [2800/5020 (56%)]\tLoss: 0.015399\n",
      "Train Epoch: 56 [3200/5020 (64%)]\tLoss: 0.125886\n",
      "Train Epoch: 56 [3600/5020 (72%)]\tLoss: 0.148107\n",
      "Train Epoch: 56 [4000/5020 (80%)]\tLoss: 0.504398\n",
      "Train Epoch: 56 [4400/5020 (88%)]\tLoss: 0.865887\n",
      "Train Epoch: 56 [4800/5020 (96%)]\tLoss: 0.639082\n",
      "\n",
      "Test set: Avg. loss: 0.4733, Accuracy: 426/519 (82%)\n",
      "\n",
      "Train Epoch: 57 [0/5020 (0%)]\tLoss: 1.304311\n",
      "Train Epoch: 57 [400/5020 (8%)]\tLoss: 1.081486\n",
      "Train Epoch: 57 [800/5020 (16%)]\tLoss: 0.442178\n",
      "Train Epoch: 57 [1200/5020 (24%)]\tLoss: 0.861327\n",
      "Train Epoch: 57 [1600/5020 (32%)]\tLoss: 0.570257\n",
      "Train Epoch: 57 [2000/5020 (40%)]\tLoss: 0.616749\n",
      "Train Epoch: 57 [2400/5020 (48%)]\tLoss: 0.319887\n",
      "Train Epoch: 57 [2800/5020 (56%)]\tLoss: 0.643677\n",
      "Train Epoch: 57 [3200/5020 (64%)]\tLoss: 0.589343\n",
      "Train Epoch: 57 [3600/5020 (72%)]\tLoss: 0.594581\n",
      "Train Epoch: 57 [4000/5020 (80%)]\tLoss: 0.298318\n",
      "Train Epoch: 57 [4400/5020 (88%)]\tLoss: 0.200487\n",
      "Train Epoch: 57 [4800/5020 (96%)]\tLoss: 0.333158\n",
      "\n",
      "Test set: Avg. loss: 0.4736, Accuracy: 426/519 (82%)\n",
      "\n",
      "Train Epoch: 58 [0/5020 (0%)]\tLoss: 0.641896\n",
      "Train Epoch: 58 [400/5020 (8%)]\tLoss: 0.945973\n",
      "Train Epoch: 58 [800/5020 (16%)]\tLoss: 0.407790\n",
      "Train Epoch: 58 [1200/5020 (24%)]\tLoss: 0.268131\n",
      "Train Epoch: 58 [1600/5020 (32%)]\tLoss: 0.539645\n",
      "Train Epoch: 58 [2000/5020 (40%)]\tLoss: 0.145571\n",
      "Train Epoch: 58 [2400/5020 (48%)]\tLoss: 0.136517\n",
      "Train Epoch: 58 [2800/5020 (56%)]\tLoss: 0.609686\n",
      "Train Epoch: 58 [3200/5020 (64%)]\tLoss: 0.723162\n",
      "Train Epoch: 58 [3600/5020 (72%)]\tLoss: 0.415484\n",
      "Train Epoch: 58 [4000/5020 (80%)]\tLoss: 0.718615\n",
      "Train Epoch: 58 [4400/5020 (88%)]\tLoss: 0.513093\n",
      "Train Epoch: 58 [4800/5020 (96%)]\tLoss: 0.259498\n",
      "\n",
      "Test set: Avg. loss: 0.4740, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 59 [0/5020 (0%)]\tLoss: 0.529555\n",
      "Train Epoch: 59 [400/5020 (8%)]\tLoss: 0.927123\n",
      "Train Epoch: 59 [800/5020 (16%)]\tLoss: 0.968716\n",
      "Train Epoch: 59 [1200/5020 (24%)]\tLoss: 0.193096\n",
      "Train Epoch: 59 [1600/5020 (32%)]\tLoss: 0.486641\n",
      "Train Epoch: 59 [2000/5020 (40%)]\tLoss: 0.119442\n",
      "Train Epoch: 59 [2400/5020 (48%)]\tLoss: 0.716205\n",
      "Train Epoch: 59 [2800/5020 (56%)]\tLoss: 0.343404\n",
      "Train Epoch: 59 [3200/5020 (64%)]\tLoss: 0.416139\n",
      "Train Epoch: 59 [3600/5020 (72%)]\tLoss: 0.903147\n",
      "Train Epoch: 59 [4000/5020 (80%)]\tLoss: 0.288898\n",
      "Train Epoch: 59 [4400/5020 (88%)]\tLoss: 0.913262\n",
      "Train Epoch: 59 [4800/5020 (96%)]\tLoss: 0.281517\n",
      "\n",
      "Test set: Avg. loss: 0.4739, Accuracy: 426/519 (82%)\n",
      "\n",
      "Train Epoch: 60 [0/5020 (0%)]\tLoss: 0.660318\n",
      "Train Epoch: 60 [400/5020 (8%)]\tLoss: 0.280764\n",
      "Train Epoch: 60 [800/5020 (16%)]\tLoss: 0.362156\n",
      "Train Epoch: 60 [1200/5020 (24%)]\tLoss: 0.306691\n",
      "Train Epoch: 60 [1600/5020 (32%)]\tLoss: 0.634227\n",
      "Train Epoch: 60 [2000/5020 (40%)]\tLoss: 1.233348\n",
      "Train Epoch: 60 [2400/5020 (48%)]\tLoss: 0.212960\n",
      "Train Epoch: 60 [2800/5020 (56%)]\tLoss: 0.319739\n",
      "Train Epoch: 60 [3200/5020 (64%)]\tLoss: 0.214370\n",
      "Train Epoch: 60 [3600/5020 (72%)]\tLoss: 0.200334\n",
      "Train Epoch: 60 [4000/5020 (80%)]\tLoss: 0.266509\n",
      "Train Epoch: 60 [4400/5020 (88%)]\tLoss: 0.890500\n",
      "Train Epoch: 60 [4800/5020 (96%)]\tLoss: 0.303042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.4735, Accuracy: 426/519 (82%)\n",
      "\n",
      "Train Epoch: 61 [0/5020 (0%)]\tLoss: 0.768580\n",
      "Train Epoch: 61 [400/5020 (8%)]\tLoss: 0.661820\n",
      "Train Epoch: 61 [800/5020 (16%)]\tLoss: 0.669182\n",
      "Train Epoch: 61 [1200/5020 (24%)]\tLoss: 0.761540\n",
      "Train Epoch: 61 [1600/5020 (32%)]\tLoss: 0.719195\n",
      "Train Epoch: 61 [2000/5020 (40%)]\tLoss: 0.544030\n",
      "Train Epoch: 61 [2400/5020 (48%)]\tLoss: 0.891243\n",
      "Train Epoch: 61 [2800/5020 (56%)]\tLoss: 0.225240\n",
      "Train Epoch: 61 [3200/5020 (64%)]\tLoss: 0.917625\n",
      "Train Epoch: 61 [3600/5020 (72%)]\tLoss: 0.424674\n",
      "Train Epoch: 61 [4000/5020 (80%)]\tLoss: 1.356202\n",
      "Train Epoch: 61 [4400/5020 (88%)]\tLoss: 0.582774\n",
      "Train Epoch: 61 [4800/5020 (96%)]\tLoss: 0.440579\n",
      "\n",
      "Test set: Avg. loss: 0.4741, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 62 [0/5020 (0%)]\tLoss: 0.236109\n",
      "Train Epoch: 62 [400/5020 (8%)]\tLoss: 0.464458\n",
      "Train Epoch: 62 [800/5020 (16%)]\tLoss: 0.279439\n",
      "Train Epoch: 62 [1200/5020 (24%)]\tLoss: 0.215346\n",
      "Train Epoch: 62 [1600/5020 (32%)]\tLoss: 0.258599\n",
      "Train Epoch: 62 [2000/5020 (40%)]\tLoss: 0.132206\n",
      "Train Epoch: 62 [2400/5020 (48%)]\tLoss: 0.208893\n",
      "Train Epoch: 62 [2800/5020 (56%)]\tLoss: 0.350272\n",
      "Train Epoch: 62 [3200/5020 (64%)]\tLoss: 0.203655\n",
      "Train Epoch: 62 [3600/5020 (72%)]\tLoss: 0.701527\n",
      "Train Epoch: 62 [4000/5020 (80%)]\tLoss: 0.172027\n",
      "Train Epoch: 62 [4400/5020 (88%)]\tLoss: 0.859977\n",
      "Train Epoch: 62 [4800/5020 (96%)]\tLoss: 0.657234\n",
      "\n",
      "Test set: Avg. loss: 0.4746, Accuracy: 424/519 (82%)\n",
      "\n",
      "Train Epoch: 63 [0/5020 (0%)]\tLoss: 1.115637\n",
      "Train Epoch: 63 [400/5020 (8%)]\tLoss: 0.452106\n",
      "Train Epoch: 63 [800/5020 (16%)]\tLoss: 0.613623\n",
      "Train Epoch: 63 [1200/5020 (24%)]\tLoss: 0.551307\n",
      "Train Epoch: 63 [1600/5020 (32%)]\tLoss: 0.569868\n",
      "Train Epoch: 63 [2000/5020 (40%)]\tLoss: 0.125089\n",
      "Train Epoch: 63 [2400/5020 (48%)]\tLoss: 0.697664\n",
      "Train Epoch: 63 [2800/5020 (56%)]\tLoss: 0.589070\n",
      "Train Epoch: 63 [3200/5020 (64%)]\tLoss: 1.094464\n",
      "Train Epoch: 63 [3600/5020 (72%)]\tLoss: 0.262515\n",
      "Train Epoch: 63 [4000/5020 (80%)]\tLoss: 0.458958\n",
      "Train Epoch: 63 [4400/5020 (88%)]\tLoss: 0.400548\n",
      "Train Epoch: 63 [4800/5020 (96%)]\tLoss: 0.414937\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 64 [0/5020 (0%)]\tLoss: 0.699768\n",
      "Train Epoch: 64 [400/5020 (8%)]\tLoss: 0.602446\n",
      "Train Epoch: 64 [800/5020 (16%)]\tLoss: 0.972495\n",
      "Train Epoch: 64 [1200/5020 (24%)]\tLoss: 0.556103\n",
      "Train Epoch: 64 [1600/5020 (32%)]\tLoss: 0.346977\n",
      "Train Epoch: 64 [2000/5020 (40%)]\tLoss: 0.192616\n",
      "Train Epoch: 64 [2400/5020 (48%)]\tLoss: 0.531801\n",
      "Train Epoch: 64 [2800/5020 (56%)]\tLoss: 0.168022\n",
      "Train Epoch: 64 [3200/5020 (64%)]\tLoss: 0.839054\n",
      "Train Epoch: 64 [3600/5020 (72%)]\tLoss: 0.481229\n",
      "Train Epoch: 64 [4000/5020 (80%)]\tLoss: 0.131274\n",
      "Train Epoch: 64 [4400/5020 (88%)]\tLoss: 0.709781\n",
      "Train Epoch: 64 [4800/5020 (96%)]\tLoss: 0.507693\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 65 [0/5020 (0%)]\tLoss: 0.691025\n",
      "Train Epoch: 65 [400/5020 (8%)]\tLoss: 0.726048\n",
      "Train Epoch: 65 [800/5020 (16%)]\tLoss: 0.425042\n",
      "Train Epoch: 65 [1200/5020 (24%)]\tLoss: 0.964532\n",
      "Train Epoch: 65 [1600/5020 (32%)]\tLoss: 0.416126\n",
      "Train Epoch: 65 [2000/5020 (40%)]\tLoss: 0.113020\n",
      "Train Epoch: 65 [2400/5020 (48%)]\tLoss: 0.558185\n",
      "Train Epoch: 65 [2800/5020 (56%)]\tLoss: 0.293809\n",
      "Train Epoch: 65 [3200/5020 (64%)]\tLoss: 0.839060\n",
      "Train Epoch: 65 [3600/5020 (72%)]\tLoss: 0.311717\n",
      "Train Epoch: 65 [4000/5020 (80%)]\tLoss: 0.474984\n",
      "Train Epoch: 65 [4400/5020 (88%)]\tLoss: 0.620291\n",
      "Train Epoch: 65 [4800/5020 (96%)]\tLoss: 0.231343\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 66 [0/5020 (0%)]\tLoss: 0.780776\n",
      "Train Epoch: 66 [400/5020 (8%)]\tLoss: 0.456447\n",
      "Train Epoch: 66 [800/5020 (16%)]\tLoss: 0.404714\n",
      "Train Epoch: 66 [1200/5020 (24%)]\tLoss: 0.631137\n",
      "Train Epoch: 66 [1600/5020 (32%)]\tLoss: 0.413799\n",
      "Train Epoch: 66 [2000/5020 (40%)]\tLoss: 0.384461\n",
      "Train Epoch: 66 [2400/5020 (48%)]\tLoss: 0.269738\n",
      "Train Epoch: 66 [2800/5020 (56%)]\tLoss: 0.411217\n",
      "Train Epoch: 66 [3200/5020 (64%)]\tLoss: 0.272564\n",
      "Train Epoch: 66 [3600/5020 (72%)]\tLoss: 0.735724\n",
      "Train Epoch: 66 [4000/5020 (80%)]\tLoss: 0.661506\n",
      "Train Epoch: 66 [4400/5020 (88%)]\tLoss: 0.674851\n",
      "Train Epoch: 66 [4800/5020 (96%)]\tLoss: 0.296710\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 67 [0/5020 (0%)]\tLoss: 0.368195\n",
      "Train Epoch: 67 [400/5020 (8%)]\tLoss: 0.347151\n",
      "Train Epoch: 67 [800/5020 (16%)]\tLoss: 0.449368\n",
      "Train Epoch: 67 [1200/5020 (24%)]\tLoss: 0.119126\n",
      "Train Epoch: 67 [1600/5020 (32%)]\tLoss: 0.335503\n",
      "Train Epoch: 67 [2000/5020 (40%)]\tLoss: 0.176574\n",
      "Train Epoch: 67 [2400/5020 (48%)]\tLoss: 0.406686\n",
      "Train Epoch: 67 [2800/5020 (56%)]\tLoss: 0.455387\n",
      "Train Epoch: 67 [3200/5020 (64%)]\tLoss: 0.210851\n",
      "Train Epoch: 67 [3600/5020 (72%)]\tLoss: 0.635911\n",
      "Train Epoch: 67 [4000/5020 (80%)]\tLoss: 0.581259\n",
      "Train Epoch: 67 [4400/5020 (88%)]\tLoss: 0.336877\n",
      "Train Epoch: 67 [4800/5020 (96%)]\tLoss: 0.291483\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 68 [0/5020 (0%)]\tLoss: 0.360414\n",
      "Train Epoch: 68 [400/5020 (8%)]\tLoss: 0.660682\n",
      "Train Epoch: 68 [800/5020 (16%)]\tLoss: 0.278612\n",
      "Train Epoch: 68 [1200/5020 (24%)]\tLoss: 0.115748\n",
      "Train Epoch: 68 [1600/5020 (32%)]\tLoss: 0.169473\n",
      "Train Epoch: 68 [2000/5020 (40%)]\tLoss: 0.559424\n",
      "Train Epoch: 68 [2400/5020 (48%)]\tLoss: 0.625491\n",
      "Train Epoch: 68 [2800/5020 (56%)]\tLoss: 0.872087\n",
      "Train Epoch: 68 [3200/5020 (64%)]\tLoss: 0.052426\n",
      "Train Epoch: 68 [3600/5020 (72%)]\tLoss: 3.362936\n",
      "Train Epoch: 68 [4000/5020 (80%)]\tLoss: 0.327600\n",
      "Train Epoch: 68 [4400/5020 (88%)]\tLoss: 0.613845\n",
      "Train Epoch: 68 [4800/5020 (96%)]\tLoss: 0.775271\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 69 [0/5020 (0%)]\tLoss: 0.389946\n",
      "Train Epoch: 69 [400/5020 (8%)]\tLoss: 0.338946\n",
      "Train Epoch: 69 [800/5020 (16%)]\tLoss: 0.649571\n",
      "Train Epoch: 69 [1200/5020 (24%)]\tLoss: 0.521835\n",
      "Train Epoch: 69 [1600/5020 (32%)]\tLoss: 0.126397\n",
      "Train Epoch: 69 [2000/5020 (40%)]\tLoss: 0.458023\n",
      "Train Epoch: 69 [2400/5020 (48%)]\tLoss: 0.932874\n",
      "Train Epoch: 69 [2800/5020 (56%)]\tLoss: 0.503506\n",
      "Train Epoch: 69 [3200/5020 (64%)]\tLoss: 0.402669\n",
      "Train Epoch: 69 [3600/5020 (72%)]\tLoss: 0.701707\n",
      "Train Epoch: 69 [4000/5020 (80%)]\tLoss: 0.790811\n",
      "Train Epoch: 69 [4400/5020 (88%)]\tLoss: 0.350616\n",
      "Train Epoch: 69 [4800/5020 (96%)]\tLoss: 0.925665\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 424/519 (82%)\n",
      "\n",
      "Train Epoch: 70 [0/5020 (0%)]\tLoss: 0.325228\n",
      "Train Epoch: 70 [400/5020 (8%)]\tLoss: 1.153957\n",
      "Train Epoch: 70 [800/5020 (16%)]\tLoss: 1.002709\n",
      "Train Epoch: 70 [1200/5020 (24%)]\tLoss: 0.482080\n",
      "Train Epoch: 70 [1600/5020 (32%)]\tLoss: 0.871299\n",
      "Train Epoch: 70 [2000/5020 (40%)]\tLoss: 0.966901\n",
      "Train Epoch: 70 [2400/5020 (48%)]\tLoss: 0.269005\n",
      "Train Epoch: 70 [2800/5020 (56%)]\tLoss: 0.676515\n",
      "Train Epoch: 70 [3200/5020 (64%)]\tLoss: 0.447471\n",
      "Train Epoch: 70 [3600/5020 (72%)]\tLoss: 0.627392\n",
      "Train Epoch: 70 [4000/5020 (80%)]\tLoss: 0.386127\n",
      "Train Epoch: 70 [4400/5020 (88%)]\tLoss: 0.061561\n",
      "Train Epoch: 70 [4800/5020 (96%)]\tLoss: 0.474634\n",
      "\n",
      "Test set: Avg. loss: 0.4745, Accuracy: 424/519 (82%)\n",
      "\n",
      "Train Epoch: 71 [0/5020 (0%)]\tLoss: 0.537257\n",
      "Train Epoch: 71 [400/5020 (8%)]\tLoss: 0.446572\n",
      "Train Epoch: 71 [800/5020 (16%)]\tLoss: 0.675602\n",
      "Train Epoch: 71 [1200/5020 (24%)]\tLoss: 0.522786\n",
      "Train Epoch: 71 [1600/5020 (32%)]\tLoss: 0.202144\n",
      "Train Epoch: 71 [2000/5020 (40%)]\tLoss: 0.360935\n",
      "Train Epoch: 71 [2400/5020 (48%)]\tLoss: 1.862817\n",
      "Train Epoch: 71 [2800/5020 (56%)]\tLoss: 0.667600\n",
      "Train Epoch: 71 [3200/5020 (64%)]\tLoss: 0.493854\n",
      "Train Epoch: 71 [3600/5020 (72%)]\tLoss: 0.026292\n",
      "Train Epoch: 71 [4000/5020 (80%)]\tLoss: 0.409652\n",
      "Train Epoch: 71 [4400/5020 (88%)]\tLoss: 0.777012\n",
      "Train Epoch: 71 [4800/5020 (96%)]\tLoss: 0.562982\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 72 [0/5020 (0%)]\tLoss: 0.370349\n",
      "Train Epoch: 72 [400/5020 (8%)]\tLoss: 0.762294\n",
      "Train Epoch: 72 [800/5020 (16%)]\tLoss: 0.576979\n",
      "Train Epoch: 72 [1200/5020 (24%)]\tLoss: 0.244279\n",
      "Train Epoch: 72 [1600/5020 (32%)]\tLoss: 0.768103\n",
      "Train Epoch: 72 [2000/5020 (40%)]\tLoss: 0.563350\n",
      "Train Epoch: 72 [2400/5020 (48%)]\tLoss: 0.828418\n",
      "Train Epoch: 72 [2800/5020 (56%)]\tLoss: 0.327656\n",
      "Train Epoch: 72 [3200/5020 (64%)]\tLoss: 0.210930\n",
      "Train Epoch: 72 [3600/5020 (72%)]\tLoss: 0.530857\n",
      "Train Epoch: 72 [4000/5020 (80%)]\tLoss: 0.445693\n",
      "Train Epoch: 72 [4400/5020 (88%)]\tLoss: 0.304652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 72 [4800/5020 (96%)]\tLoss: 0.530374\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 424/519 (82%)\n",
      "\n",
      "Train Epoch: 73 [0/5020 (0%)]\tLoss: 0.262284\n",
      "Train Epoch: 73 [400/5020 (8%)]\tLoss: 0.586823\n",
      "Train Epoch: 73 [800/5020 (16%)]\tLoss: 1.130450\n",
      "Train Epoch: 73 [1200/5020 (24%)]\tLoss: 0.539847\n",
      "Train Epoch: 73 [1600/5020 (32%)]\tLoss: 0.575984\n",
      "Train Epoch: 73 [2000/5020 (40%)]\tLoss: 0.907579\n",
      "Train Epoch: 73 [2400/5020 (48%)]\tLoss: 0.631077\n",
      "Train Epoch: 73 [2800/5020 (56%)]\tLoss: 0.630522\n",
      "Train Epoch: 73 [3200/5020 (64%)]\tLoss: 0.461156\n",
      "Train Epoch: 73 [3600/5020 (72%)]\tLoss: 1.723468\n",
      "Train Epoch: 73 [4000/5020 (80%)]\tLoss: 0.344304\n",
      "Train Epoch: 73 [4400/5020 (88%)]\tLoss: 0.092959\n",
      "Train Epoch: 73 [4800/5020 (96%)]\tLoss: 0.888513\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 74 [0/5020 (0%)]\tLoss: 0.293400\n",
      "Train Epoch: 74 [400/5020 (8%)]\tLoss: 1.183713\n",
      "Train Epoch: 74 [800/5020 (16%)]\tLoss: 0.748905\n",
      "Train Epoch: 74 [1200/5020 (24%)]\tLoss: 0.645527\n",
      "Train Epoch: 74 [1600/5020 (32%)]\tLoss: 0.382694\n",
      "Train Epoch: 74 [2000/5020 (40%)]\tLoss: 0.025199\n",
      "Train Epoch: 74 [2400/5020 (48%)]\tLoss: 0.740476\n",
      "Train Epoch: 74 [2800/5020 (56%)]\tLoss: 0.317624\n",
      "Train Epoch: 74 [3200/5020 (64%)]\tLoss: 0.751994\n",
      "Train Epoch: 74 [3600/5020 (72%)]\tLoss: 0.089711\n",
      "Train Epoch: 74 [4000/5020 (80%)]\tLoss: 0.520924\n",
      "Train Epoch: 74 [4400/5020 (88%)]\tLoss: 0.416414\n",
      "Train Epoch: 74 [4800/5020 (96%)]\tLoss: 0.356347\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 75 [0/5020 (0%)]\tLoss: 0.258362\n",
      "Train Epoch: 75 [400/5020 (8%)]\tLoss: 0.628125\n",
      "Train Epoch: 75 [800/5020 (16%)]\tLoss: 0.282551\n",
      "Train Epoch: 75 [1200/5020 (24%)]\tLoss: 0.784139\n",
      "Train Epoch: 75 [1600/5020 (32%)]\tLoss: 0.777082\n",
      "Train Epoch: 75 [2000/5020 (40%)]\tLoss: 1.598483\n",
      "Train Epoch: 75 [2400/5020 (48%)]\tLoss: 1.043686\n",
      "Train Epoch: 75 [2800/5020 (56%)]\tLoss: 0.005539\n",
      "Train Epoch: 75 [3200/5020 (64%)]\tLoss: 0.769235\n",
      "Train Epoch: 75 [3600/5020 (72%)]\tLoss: 0.201561\n",
      "Train Epoch: 75 [4000/5020 (80%)]\tLoss: 0.370545\n",
      "Train Epoch: 75 [4400/5020 (88%)]\tLoss: 0.909578\n",
      "Train Epoch: 75 [4800/5020 (96%)]\tLoss: 0.397379\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 76 [0/5020 (0%)]\tLoss: 0.630226\n",
      "Train Epoch: 76 [400/5020 (8%)]\tLoss: 0.546576\n",
      "Train Epoch: 76 [800/5020 (16%)]\tLoss: 0.508011\n",
      "Train Epoch: 76 [1200/5020 (24%)]\tLoss: 1.043864\n",
      "Train Epoch: 76 [1600/5020 (32%)]\tLoss: 0.801649\n",
      "Train Epoch: 76 [2000/5020 (40%)]\tLoss: 0.783368\n",
      "Train Epoch: 76 [2400/5020 (48%)]\tLoss: 0.812216\n",
      "Train Epoch: 76 [2800/5020 (56%)]\tLoss: 0.307987\n",
      "Train Epoch: 76 [3200/5020 (64%)]\tLoss: 0.556737\n",
      "Train Epoch: 76 [3600/5020 (72%)]\tLoss: 0.319483\n",
      "Train Epoch: 76 [4000/5020 (80%)]\tLoss: 0.377838\n",
      "Train Epoch: 76 [4400/5020 (88%)]\tLoss: 0.016357\n",
      "Train Epoch: 76 [4800/5020 (96%)]\tLoss: 0.382839\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 77 [0/5020 (0%)]\tLoss: 0.802712\n",
      "Train Epoch: 77 [400/5020 (8%)]\tLoss: 0.517114\n",
      "Train Epoch: 77 [800/5020 (16%)]\tLoss: 0.904812\n",
      "Train Epoch: 77 [1200/5020 (24%)]\tLoss: 0.767633\n",
      "Train Epoch: 77 [1600/5020 (32%)]\tLoss: 0.246122\n",
      "Train Epoch: 77 [2000/5020 (40%)]\tLoss: 0.276386\n",
      "Train Epoch: 77 [2400/5020 (48%)]\tLoss: 0.387545\n",
      "Train Epoch: 77 [2800/5020 (56%)]\tLoss: 0.569303\n",
      "Train Epoch: 77 [3200/5020 (64%)]\tLoss: 0.142270\n",
      "Train Epoch: 77 [3600/5020 (72%)]\tLoss: 0.260331\n",
      "Train Epoch: 77 [4000/5020 (80%)]\tLoss: 0.492887\n",
      "Train Epoch: 77 [4400/5020 (88%)]\tLoss: 0.253025\n",
      "Train Epoch: 77 [4800/5020 (96%)]\tLoss: 0.803716\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 78 [0/5020 (0%)]\tLoss: 0.442001\n",
      "Train Epoch: 78 [400/5020 (8%)]\tLoss: 0.507048\n",
      "Train Epoch: 78 [800/5020 (16%)]\tLoss: 0.492936\n",
      "Train Epoch: 78 [1200/5020 (24%)]\tLoss: 0.801101\n",
      "Train Epoch: 78 [1600/5020 (32%)]\tLoss: 0.229600\n",
      "Train Epoch: 78 [2000/5020 (40%)]\tLoss: 0.017616\n",
      "Train Epoch: 78 [2400/5020 (48%)]\tLoss: 0.322716\n",
      "Train Epoch: 78 [2800/5020 (56%)]\tLoss: 0.564431\n",
      "Train Epoch: 78 [3200/5020 (64%)]\tLoss: 0.727830\n",
      "Train Epoch: 78 [3600/5020 (72%)]\tLoss: 0.240696\n",
      "Train Epoch: 78 [4000/5020 (80%)]\tLoss: 0.639911\n",
      "Train Epoch: 78 [4400/5020 (88%)]\tLoss: 0.602193\n",
      "Train Epoch: 78 [4800/5020 (96%)]\tLoss: 0.469553\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 79 [0/5020 (0%)]\tLoss: 0.556153\n",
      "Train Epoch: 79 [400/5020 (8%)]\tLoss: 0.075895\n",
      "Train Epoch: 79 [800/5020 (16%)]\tLoss: 0.368037\n",
      "Train Epoch: 79 [1200/5020 (24%)]\tLoss: 0.503198\n",
      "Train Epoch: 79 [1600/5020 (32%)]\tLoss: 1.152148\n",
      "Train Epoch: 79 [2000/5020 (40%)]\tLoss: 0.796865\n",
      "Train Epoch: 79 [2400/5020 (48%)]\tLoss: 0.940174\n",
      "Train Epoch: 79 [2800/5020 (56%)]\tLoss: 0.364250\n",
      "Train Epoch: 79 [3200/5020 (64%)]\tLoss: 0.521648\n",
      "Train Epoch: 79 [3600/5020 (72%)]\tLoss: 0.648819\n",
      "Train Epoch: 79 [4000/5020 (80%)]\tLoss: 0.212227\n",
      "Train Epoch: 79 [4400/5020 (88%)]\tLoss: 0.283771\n",
      "Train Epoch: 79 [4800/5020 (96%)]\tLoss: 0.360721\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 80 [0/5020 (0%)]\tLoss: 0.530820\n",
      "Train Epoch: 80 [400/5020 (8%)]\tLoss: 0.426848\n",
      "Train Epoch: 80 [800/5020 (16%)]\tLoss: 0.405927\n",
      "Train Epoch: 80 [1200/5020 (24%)]\tLoss: 0.802412\n",
      "Train Epoch: 80 [1600/5020 (32%)]\tLoss: 0.836362\n",
      "Train Epoch: 80 [2000/5020 (40%)]\tLoss: 0.394038\n",
      "Train Epoch: 80 [2400/5020 (48%)]\tLoss: 0.258436\n",
      "Train Epoch: 80 [2800/5020 (56%)]\tLoss: 0.218970\n",
      "Train Epoch: 80 [3200/5020 (64%)]\tLoss: 0.267354\n",
      "Train Epoch: 80 [3600/5020 (72%)]\tLoss: 0.811773\n",
      "Train Epoch: 80 [4000/5020 (80%)]\tLoss: 0.666931\n",
      "Train Epoch: 80 [4400/5020 (88%)]\tLoss: 0.843181\n",
      "Train Epoch: 80 [4800/5020 (96%)]\tLoss: 0.398538\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 81 [0/5020 (0%)]\tLoss: 0.572870\n",
      "Train Epoch: 81 [400/5020 (8%)]\tLoss: 0.467645\n",
      "Train Epoch: 81 [800/5020 (16%)]\tLoss: 0.291066\n",
      "Train Epoch: 81 [1200/5020 (24%)]\tLoss: 0.787483\n",
      "Train Epoch: 81 [1600/5020 (32%)]\tLoss: 0.550689\n",
      "Train Epoch: 81 [2000/5020 (40%)]\tLoss: 0.462086\n",
      "Train Epoch: 81 [2400/5020 (48%)]\tLoss: 0.247535\n",
      "Train Epoch: 81 [2800/5020 (56%)]\tLoss: 0.433206\n",
      "Train Epoch: 81 [3200/5020 (64%)]\tLoss: 0.603281\n",
      "Train Epoch: 81 [3600/5020 (72%)]\tLoss: 0.782765\n",
      "Train Epoch: 81 [4000/5020 (80%)]\tLoss: 0.585152\n",
      "Train Epoch: 81 [4400/5020 (88%)]\tLoss: 0.353230\n",
      "Train Epoch: 81 [4800/5020 (96%)]\tLoss: 0.708406\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 82 [0/5020 (0%)]\tLoss: 0.159976\n",
      "Train Epoch: 82 [400/5020 (8%)]\tLoss: 0.766270\n",
      "Train Epoch: 82 [800/5020 (16%)]\tLoss: 0.654224\n",
      "Train Epoch: 82 [1200/5020 (24%)]\tLoss: 0.234669\n",
      "Train Epoch: 82 [1600/5020 (32%)]\tLoss: 1.148703\n",
      "Train Epoch: 82 [2000/5020 (40%)]\tLoss: 0.779226\n",
      "Train Epoch: 82 [2400/5020 (48%)]\tLoss: 0.616449\n",
      "Train Epoch: 82 [2800/5020 (56%)]\tLoss: 0.142737\n",
      "Train Epoch: 82 [3200/5020 (64%)]\tLoss: 0.799677\n",
      "Train Epoch: 82 [3600/5020 (72%)]\tLoss: 0.767702\n",
      "Train Epoch: 82 [4000/5020 (80%)]\tLoss: 0.633145\n",
      "Train Epoch: 82 [4400/5020 (88%)]\tLoss: 0.332911\n",
      "Train Epoch: 82 [4800/5020 (96%)]\tLoss: 0.642263\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 83 [0/5020 (0%)]\tLoss: 0.311970\n",
      "Train Epoch: 83 [400/5020 (8%)]\tLoss: 0.146172\n",
      "Train Epoch: 83 [800/5020 (16%)]\tLoss: 0.060386\n",
      "Train Epoch: 83 [1200/5020 (24%)]\tLoss: 1.745483\n",
      "Train Epoch: 83 [1600/5020 (32%)]\tLoss: 0.507749\n",
      "Train Epoch: 83 [2000/5020 (40%)]\tLoss: 0.769474\n",
      "Train Epoch: 83 [2400/5020 (48%)]\tLoss: 0.629477\n",
      "Train Epoch: 83 [2800/5020 (56%)]\tLoss: 0.565615\n",
      "Train Epoch: 83 [3200/5020 (64%)]\tLoss: 0.225300\n",
      "Train Epoch: 83 [3600/5020 (72%)]\tLoss: 0.779237\n",
      "Train Epoch: 83 [4000/5020 (80%)]\tLoss: 1.790500\n",
      "Train Epoch: 83 [4400/5020 (88%)]\tLoss: 0.476156\n",
      "Train Epoch: 83 [4800/5020 (96%)]\tLoss: 0.122973\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 84 [0/5020 (0%)]\tLoss: 0.562672\n",
      "Train Epoch: 84 [400/5020 (8%)]\tLoss: 1.065289\n",
      "Train Epoch: 84 [800/5020 (16%)]\tLoss: 0.105831\n",
      "Train Epoch: 84 [1200/5020 (24%)]\tLoss: 0.486033\n",
      "Train Epoch: 84 [1600/5020 (32%)]\tLoss: 0.327951\n",
      "Train Epoch: 84 [2000/5020 (40%)]\tLoss: 0.552800\n",
      "Train Epoch: 84 [2400/5020 (48%)]\tLoss: 0.704612\n",
      "Train Epoch: 84 [2800/5020 (56%)]\tLoss: 1.100954\n",
      "Train Epoch: 84 [3200/5020 (64%)]\tLoss: 0.405443\n",
      "Train Epoch: 84 [3600/5020 (72%)]\tLoss: 0.830730\n",
      "Train Epoch: 84 [4000/5020 (80%)]\tLoss: 0.240961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 84 [4400/5020 (88%)]\tLoss: 0.507346\n",
      "Train Epoch: 84 [4800/5020 (96%)]\tLoss: 0.056324\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 85 [0/5020 (0%)]\tLoss: 0.315054\n",
      "Train Epoch: 85 [400/5020 (8%)]\tLoss: 0.843788\n",
      "Train Epoch: 85 [800/5020 (16%)]\tLoss: 0.138077\n",
      "Train Epoch: 85 [1200/5020 (24%)]\tLoss: 0.630585\n",
      "Train Epoch: 85 [1600/5020 (32%)]\tLoss: 1.623685\n",
      "Train Epoch: 85 [2000/5020 (40%)]\tLoss: 0.781478\n",
      "Train Epoch: 85 [2400/5020 (48%)]\tLoss: 0.544907\n",
      "Train Epoch: 85 [2800/5020 (56%)]\tLoss: 1.055612\n",
      "Train Epoch: 85 [3200/5020 (64%)]\tLoss: 0.656052\n",
      "Train Epoch: 85 [3600/5020 (72%)]\tLoss: 0.429478\n",
      "Train Epoch: 85 [4000/5020 (80%)]\tLoss: 0.594119\n",
      "Train Epoch: 85 [4400/5020 (88%)]\tLoss: 0.604450\n",
      "Train Epoch: 85 [4800/5020 (96%)]\tLoss: 0.381408\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 86 [0/5020 (0%)]\tLoss: 0.356893\n",
      "Train Epoch: 86 [400/5020 (8%)]\tLoss: 0.015682\n",
      "Train Epoch: 86 [800/5020 (16%)]\tLoss: 0.408465\n",
      "Train Epoch: 86 [1200/5020 (24%)]\tLoss: 0.408300\n",
      "Train Epoch: 86 [1600/5020 (32%)]\tLoss: 0.849330\n",
      "Train Epoch: 86 [2000/5020 (40%)]\tLoss: 0.254123\n",
      "Train Epoch: 86 [2400/5020 (48%)]\tLoss: 0.469298\n",
      "Train Epoch: 86 [2800/5020 (56%)]\tLoss: 0.956363\n",
      "Train Epoch: 86 [3200/5020 (64%)]\tLoss: 0.976921\n",
      "Train Epoch: 86 [3600/5020 (72%)]\tLoss: 1.217942\n",
      "Train Epoch: 86 [4000/5020 (80%)]\tLoss: 0.451318\n",
      "Train Epoch: 86 [4400/5020 (88%)]\tLoss: 0.330125\n",
      "Train Epoch: 86 [4800/5020 (96%)]\tLoss: 0.832532\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 87 [0/5020 (0%)]\tLoss: 0.427764\n",
      "Train Epoch: 87 [400/5020 (8%)]\tLoss: 0.501292\n",
      "Train Epoch: 87 [800/5020 (16%)]\tLoss: 0.272048\n",
      "Train Epoch: 87 [1200/5020 (24%)]\tLoss: 0.475244\n",
      "Train Epoch: 87 [1600/5020 (32%)]\tLoss: 0.574062\n",
      "Train Epoch: 87 [2000/5020 (40%)]\tLoss: 1.216016\n",
      "Train Epoch: 87 [2400/5020 (48%)]\tLoss: 0.470338\n",
      "Train Epoch: 87 [2800/5020 (56%)]\tLoss: 0.452983\n",
      "Train Epoch: 87 [3200/5020 (64%)]\tLoss: 0.420968\n",
      "Train Epoch: 87 [3600/5020 (72%)]\tLoss: 0.633554\n",
      "Train Epoch: 87 [4000/5020 (80%)]\tLoss: 0.295989\n",
      "Train Epoch: 87 [4400/5020 (88%)]\tLoss: 0.225747\n",
      "Train Epoch: 87 [4800/5020 (96%)]\tLoss: 0.690091\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 88 [0/5020 (0%)]\tLoss: 0.728791\n",
      "Train Epoch: 88 [400/5020 (8%)]\tLoss: 0.114026\n",
      "Train Epoch: 88 [800/5020 (16%)]\tLoss: 0.942589\n",
      "Train Epoch: 88 [1200/5020 (24%)]\tLoss: 0.942824\n",
      "Train Epoch: 88 [1600/5020 (32%)]\tLoss: 0.691909\n",
      "Train Epoch: 88 [2000/5020 (40%)]\tLoss: 0.659271\n",
      "Train Epoch: 88 [2400/5020 (48%)]\tLoss: 0.234712\n",
      "Train Epoch: 88 [2800/5020 (56%)]\tLoss: 0.592872\n",
      "Train Epoch: 88 [3200/5020 (64%)]\tLoss: 0.520327\n",
      "Train Epoch: 88 [3600/5020 (72%)]\tLoss: 0.528204\n",
      "Train Epoch: 88 [4000/5020 (80%)]\tLoss: 0.174208\n",
      "Train Epoch: 88 [4400/5020 (88%)]\tLoss: 0.442846\n",
      "Train Epoch: 88 [4800/5020 (96%)]\tLoss: 0.197815\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-08.\n",
      "\n",
      "Test set: Avg. loss: 0.4743, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 89 [0/5020 (0%)]\tLoss: 0.703186\n",
      "Train Epoch: 89 [400/5020 (8%)]\tLoss: 0.849402\n",
      "Train Epoch: 89 [800/5020 (16%)]\tLoss: 0.365049\n",
      "Train Epoch: 89 [1200/5020 (24%)]\tLoss: 0.208293\n",
      "Train Epoch: 89 [1600/5020 (32%)]\tLoss: 0.361886\n",
      "Train Epoch: 89 [2000/5020 (40%)]\tLoss: 0.472240\n",
      "Train Epoch: 89 [2400/5020 (48%)]\tLoss: 1.004617\n",
      "Train Epoch: 89 [2800/5020 (56%)]\tLoss: 0.559701\n",
      "Train Epoch: 89 [3200/5020 (64%)]\tLoss: 0.770007\n",
      "Train Epoch: 89 [3600/5020 (72%)]\tLoss: 0.059350\n",
      "Train Epoch: 89 [4000/5020 (80%)]\tLoss: 0.761928\n",
      "Train Epoch: 89 [4400/5020 (88%)]\tLoss: 0.641686\n",
      "Train Epoch: 89 [4800/5020 (96%)]\tLoss: 0.650318\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 90 [0/5020 (0%)]\tLoss: 0.747680\n",
      "Train Epoch: 90 [400/5020 (8%)]\tLoss: 0.299846\n",
      "Train Epoch: 90 [800/5020 (16%)]\tLoss: 0.491720\n",
      "Train Epoch: 90 [1200/5020 (24%)]\tLoss: 0.212139\n",
      "Train Epoch: 90 [1600/5020 (32%)]\tLoss: 0.937327\n",
      "Train Epoch: 90 [2000/5020 (40%)]\tLoss: 0.218847\n",
      "Train Epoch: 90 [2400/5020 (48%)]\tLoss: 0.452550\n",
      "Train Epoch: 90 [2800/5020 (56%)]\tLoss: 0.284148\n",
      "Train Epoch: 90 [3200/5020 (64%)]\tLoss: 0.419759\n",
      "Train Epoch: 90 [3600/5020 (72%)]\tLoss: 0.531668\n",
      "Train Epoch: 90 [4000/5020 (80%)]\tLoss: 0.500223\n",
      "Train Epoch: 90 [4400/5020 (88%)]\tLoss: 0.738845\n",
      "Train Epoch: 90 [4800/5020 (96%)]\tLoss: 0.563041\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 91 [0/5020 (0%)]\tLoss: 0.248828\n",
      "Train Epoch: 91 [400/5020 (8%)]\tLoss: 0.496442\n",
      "Train Epoch: 91 [800/5020 (16%)]\tLoss: 0.804859\n",
      "Train Epoch: 91 [1200/5020 (24%)]\tLoss: 1.103816\n",
      "Train Epoch: 91 [1600/5020 (32%)]\tLoss: 0.616487\n",
      "Train Epoch: 91 [2000/5020 (40%)]\tLoss: 0.959913\n",
      "Train Epoch: 91 [2400/5020 (48%)]\tLoss: 1.183071\n",
      "Train Epoch: 91 [2800/5020 (56%)]\tLoss: 0.581822\n",
      "Train Epoch: 91 [3200/5020 (64%)]\tLoss: 0.367912\n",
      "Train Epoch: 91 [3600/5020 (72%)]\tLoss: 0.225815\n",
      "Train Epoch: 91 [4000/5020 (80%)]\tLoss: 0.721087\n",
      "Train Epoch: 91 [4400/5020 (88%)]\tLoss: 1.045935\n",
      "Train Epoch: 91 [4800/5020 (96%)]\tLoss: 0.695085\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 92 [0/5020 (0%)]\tLoss: 0.356624\n",
      "Train Epoch: 92 [400/5020 (8%)]\tLoss: 0.009705\n",
      "Train Epoch: 92 [800/5020 (16%)]\tLoss: 0.496711\n",
      "Train Epoch: 92 [1200/5020 (24%)]\tLoss: 0.919058\n",
      "Train Epoch: 92 [1600/5020 (32%)]\tLoss: 0.329780\n",
      "Train Epoch: 92 [2000/5020 (40%)]\tLoss: 0.641124\n",
      "Train Epoch: 92 [2400/5020 (48%)]\tLoss: 0.496574\n",
      "Train Epoch: 92 [2800/5020 (56%)]\tLoss: 0.619884\n",
      "Train Epoch: 92 [3200/5020 (64%)]\tLoss: 0.894338\n",
      "Train Epoch: 92 [3600/5020 (72%)]\tLoss: 0.742205\n",
      "Train Epoch: 92 [4000/5020 (80%)]\tLoss: 0.921434\n",
      "Train Epoch: 92 [4400/5020 (88%)]\tLoss: 0.624512\n",
      "Train Epoch: 92 [4800/5020 (96%)]\tLoss: 0.133810\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 93 [0/5020 (0%)]\tLoss: 1.075884\n",
      "Train Epoch: 93 [400/5020 (8%)]\tLoss: 0.402400\n",
      "Train Epoch: 93 [800/5020 (16%)]\tLoss: 0.086099\n",
      "Train Epoch: 93 [1200/5020 (24%)]\tLoss: 0.356975\n",
      "Train Epoch: 93 [1600/5020 (32%)]\tLoss: 0.171690\n",
      "Train Epoch: 93 [2000/5020 (40%)]\tLoss: 0.859419\n",
      "Train Epoch: 93 [2400/5020 (48%)]\tLoss: 0.383365\n",
      "Train Epoch: 93 [2800/5020 (56%)]\tLoss: 0.286789\n",
      "Train Epoch: 93 [3200/5020 (64%)]\tLoss: 1.064129\n",
      "Train Epoch: 93 [3600/5020 (72%)]\tLoss: 0.419688\n",
      "Train Epoch: 93 [4000/5020 (80%)]\tLoss: 0.458840\n",
      "Train Epoch: 93 [4400/5020 (88%)]\tLoss: 0.265112\n",
      "Train Epoch: 93 [4800/5020 (96%)]\tLoss: 0.233792\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 94 [0/5020 (0%)]\tLoss: 0.691402\n",
      "Train Epoch: 94 [400/5020 (8%)]\tLoss: 0.289853\n",
      "Train Epoch: 94 [800/5020 (16%)]\tLoss: 0.546818\n",
      "Train Epoch: 94 [1200/5020 (24%)]\tLoss: 0.663888\n",
      "Train Epoch: 94 [1600/5020 (32%)]\tLoss: 0.114682\n",
      "Train Epoch: 94 [2000/5020 (40%)]\tLoss: 0.829647\n",
      "Train Epoch: 94 [2400/5020 (48%)]\tLoss: 0.541244\n",
      "Train Epoch: 94 [2800/5020 (56%)]\tLoss: 0.785402\n",
      "Train Epoch: 94 [3200/5020 (64%)]\tLoss: 0.500818\n",
      "Train Epoch: 94 [3600/5020 (72%)]\tLoss: 0.648232\n",
      "Train Epoch: 94 [4000/5020 (80%)]\tLoss: 0.154527\n",
      "Train Epoch: 94 [4400/5020 (88%)]\tLoss: 1.103219\n",
      "Train Epoch: 94 [4800/5020 (96%)]\tLoss: 0.355205\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 95 [0/5020 (0%)]\tLoss: 0.928498\n",
      "Train Epoch: 95 [400/5020 (8%)]\tLoss: 0.300030\n",
      "Train Epoch: 95 [800/5020 (16%)]\tLoss: 0.591316\n",
      "Train Epoch: 95 [1200/5020 (24%)]\tLoss: 0.386613\n",
      "Train Epoch: 95 [1600/5020 (32%)]\tLoss: 0.528489\n",
      "Train Epoch: 95 [2000/5020 (40%)]\tLoss: 0.255959\n",
      "Train Epoch: 95 [2400/5020 (48%)]\tLoss: 0.261044\n",
      "Train Epoch: 95 [2800/5020 (56%)]\tLoss: 0.114686\n",
      "Train Epoch: 95 [3200/5020 (64%)]\tLoss: 0.472920\n",
      "Train Epoch: 95 [3600/5020 (72%)]\tLoss: 0.159572\n",
      "Train Epoch: 95 [4000/5020 (80%)]\tLoss: 1.592853\n",
      "Train Epoch: 95 [4400/5020 (88%)]\tLoss: 1.067814\n",
      "Train Epoch: 95 [4800/5020 (96%)]\tLoss: 1.043024\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 96 [0/5020 (0%)]\tLoss: 0.315573\n",
      "Train Epoch: 96 [400/5020 (8%)]\tLoss: 0.862069\n",
      "Train Epoch: 96 [800/5020 (16%)]\tLoss: 0.881992\n",
      "Train Epoch: 96 [1200/5020 (24%)]\tLoss: 0.783782\n",
      "Train Epoch: 96 [1600/5020 (32%)]\tLoss: 0.291128\n",
      "Train Epoch: 96 [2000/5020 (40%)]\tLoss: 0.933684\n",
      "Train Epoch: 96 [2400/5020 (48%)]\tLoss: 0.828049\n",
      "Train Epoch: 96 [2800/5020 (56%)]\tLoss: 0.554036\n",
      "Train Epoch: 96 [3200/5020 (64%)]\tLoss: 0.696827\n",
      "Train Epoch: 96 [3600/5020 (72%)]\tLoss: 0.348332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 96 [4000/5020 (80%)]\tLoss: 0.386474\n",
      "Train Epoch: 96 [4400/5020 (88%)]\tLoss: 0.334049\n",
      "Train Epoch: 96 [4800/5020 (96%)]\tLoss: 0.149398\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 97 [0/5020 (0%)]\tLoss: 0.114452\n",
      "Train Epoch: 97 [400/5020 (8%)]\tLoss: 0.652653\n",
      "Train Epoch: 97 [800/5020 (16%)]\tLoss: 0.464677\n",
      "Train Epoch: 97 [1200/5020 (24%)]\tLoss: 0.171317\n",
      "Train Epoch: 97 [1600/5020 (32%)]\tLoss: 0.662031\n",
      "Train Epoch: 97 [2000/5020 (40%)]\tLoss: 0.498985\n",
      "Train Epoch: 97 [2400/5020 (48%)]\tLoss: 0.070601\n",
      "Train Epoch: 97 [2800/5020 (56%)]\tLoss: 0.654311\n",
      "Train Epoch: 97 [3200/5020 (64%)]\tLoss: 0.927282\n",
      "Train Epoch: 97 [3600/5020 (72%)]\tLoss: 1.357441\n",
      "Train Epoch: 97 [4000/5020 (80%)]\tLoss: 0.123838\n",
      "Train Epoch: 97 [4400/5020 (88%)]\tLoss: 0.831735\n",
      "Train Epoch: 97 [4800/5020 (96%)]\tLoss: 0.419057\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 98 [0/5020 (0%)]\tLoss: 0.159417\n",
      "Train Epoch: 98 [400/5020 (8%)]\tLoss: 0.918545\n",
      "Train Epoch: 98 [800/5020 (16%)]\tLoss: 0.711160\n",
      "Train Epoch: 98 [1200/5020 (24%)]\tLoss: 0.635137\n",
      "Train Epoch: 98 [1600/5020 (32%)]\tLoss: 0.393700\n",
      "Train Epoch: 98 [2000/5020 (40%)]\tLoss: 0.286863\n",
      "Train Epoch: 98 [2400/5020 (48%)]\tLoss: 0.466187\n",
      "Train Epoch: 98 [2800/5020 (56%)]\tLoss: 0.133895\n",
      "Train Epoch: 98 [3200/5020 (64%)]\tLoss: 0.221060\n",
      "Train Epoch: 98 [3600/5020 (72%)]\tLoss: 0.413411\n",
      "Train Epoch: 98 [4000/5020 (80%)]\tLoss: 0.974959\n",
      "Train Epoch: 98 [4400/5020 (88%)]\tLoss: 0.393428\n",
      "Train Epoch: 98 [4800/5020 (96%)]\tLoss: 0.470468\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 99 [0/5020 (0%)]\tLoss: 0.700814\n",
      "Train Epoch: 99 [400/5020 (8%)]\tLoss: 0.365187\n",
      "Train Epoch: 99 [800/5020 (16%)]\tLoss: 0.415927\n",
      "Train Epoch: 99 [1200/5020 (24%)]\tLoss: 1.022394\n",
      "Train Epoch: 99 [1600/5020 (32%)]\tLoss: 0.925707\n",
      "Train Epoch: 99 [2000/5020 (40%)]\tLoss: 0.582138\n",
      "Train Epoch: 99 [2400/5020 (48%)]\tLoss: 0.573993\n",
      "Train Epoch: 99 [2800/5020 (56%)]\tLoss: 0.613294\n",
      "Train Epoch: 99 [3200/5020 (64%)]\tLoss: 0.117482\n",
      "Train Epoch: 99 [3600/5020 (72%)]\tLoss: 0.476203\n",
      "Train Epoch: 99 [4000/5020 (80%)]\tLoss: 0.325558\n",
      "Train Epoch: 99 [4400/5020 (88%)]\tLoss: 0.410559\n",
      "Train Epoch: 99 [4800/5020 (96%)]\tLoss: 0.472112\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "Train Epoch: 100 [0/5020 (0%)]\tLoss: 0.618951\n",
      "Train Epoch: 100 [400/5020 (8%)]\tLoss: 0.303737\n",
      "Train Epoch: 100 [800/5020 (16%)]\tLoss: 0.522813\n",
      "Train Epoch: 100 [1200/5020 (24%)]\tLoss: 0.499051\n",
      "Train Epoch: 100 [1600/5020 (32%)]\tLoss: 0.210620\n",
      "Train Epoch: 100 [2000/5020 (40%)]\tLoss: 0.981403\n",
      "Train Epoch: 100 [2400/5020 (48%)]\tLoss: 0.235981\n",
      "Train Epoch: 100 [2800/5020 (56%)]\tLoss: 0.402905\n",
      "Train Epoch: 100 [3200/5020 (64%)]\tLoss: 0.260109\n",
      "Train Epoch: 100 [3600/5020 (72%)]\tLoss: 0.507434\n",
      "Train Epoch: 100 [4000/5020 (80%)]\tLoss: 0.830846\n",
      "Train Epoch: 100 [4400/5020 (88%)]\tLoss: 0.241624\n",
      "Train Epoch: 100 [4800/5020 (96%)]\tLoss: 0.055675\n",
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/dev/alto-fontstyle-classifier/env/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.4744, Accuracy: 425/519 (82%)\n",
      "\n",
      "{0: '_', 1: 'bold', 2: 'italics'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.68      0.88      0.77       173\n",
      "        bold       0.96      0.88      0.92       173\n",
      "     italics       0.88      0.69      0.77       173\n",
      "\n",
      "    accuracy                           0.82       519\n",
      "   macro avg       0.84      0.82      0.82       519\n",
      "weighted avg       0.84      0.82      0.82       519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    targets, preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            targets.extend(target.tolist())\n",
    "            preds.extend(pred.tolist())\n",
    "      \n",
    "    test_loss /= len(testloader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "          test_loss, correct, len(testloader.dataset),\n",
    "          100. * correct / len(testloader.dataset)))\n",
    "    return targets, [int(x[0]) for x in preds]\n",
    "gts, preds = test()\n",
    "from sklearn.metrics import classification_report\n",
    "print(classes)\n",
    "print(classification_report(y_true=gts, y_pred=preds, target_names=[classes[i] for i in range(len(classes))]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
